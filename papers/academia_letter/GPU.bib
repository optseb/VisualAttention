
@article{cope_spinecreator:_2016,
	title = {{SpineCreator}: a {Graphical} {User} {Interface} for the {Creation} of {Layered} {Neural} {Models}},
	issn = {1539-2791, 1559-0089},
	shorttitle = {{SpineCreator}},
	url = {http://link.springer.com/10.1007/s12021-016-9311-z},
	doi = {10.1007/s12021-016-9311-z},
	language = {en},
	urldate = {2016-10-04},
	journal = {Neuroinformatics},
	author = {Cope, A. J. and Richmond, P. and James, S. S. and Gurney, K. and Allerton, D. J.},
	month = sep,
	year = {2016},
	file = {Cope et al. - 2016 - SpineCreator a Graphical User Interface for the C.pdf:/home/seb/Zotero/storage/R5ZSF4F8/Cope et al. - 2016 - SpineCreator a Graphical User Interface for the C.pdf:application/pdf}
}

@misc{cope_spinecreator_2015,
	title = {{SpineCreator}},
	copyright = {GNU GPL},
	url = {https://github.com/SpineML/SpineCreator},
	abstract = {A Graphical User Interface for the development of neural network models which writes the models out in the SpineML declarative markup language.},
	author = {Cope, A. J. and Richmond, P. and James, S. S.},
	year = {2015},
	note = {RRID: SCR\_015637}
}

@article{stimberg_brian2genn_2020,
	title = {{Brian2GeNN}: accelerating spiking neural network simulations with graphics hardware},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	shorttitle = {{Brian2GeNN}},
	url = {https://www.nature.com/articles/s41598-019-54957-7},
	doi = {10.1038/s41598-019-54957-7},
	abstract = {“Brian” is a popular Python-based simulator for spiking neural networks, commonly used in computational neuroscience. GeNN is a C++-based meta-compiler for accelerating spiking neural network simulations using consumer or high performance grade graphics processing units (GPUs). Here we introduce a new software package, Brian2GeNN, that connects the two systems so that users can make use of GeNN GPU acceleration when developing their models in Brian, without requiring any technical knowledge about GPUs, C++ or GeNN. The new Brian2GeNN software uses a pipeline of code generation to translate Brian scripts into C++ code that can be used as input to GeNN, and subsequently can be run on suitable NVIDIA GPU accelerators. From the user’s perspective, the entire pipeline is invoked by adding two simple lines to their Brian scripts. We have shown that using Brian2GeNN, two non-trivial models from the literature can run tens to hundreds of times faster than on CPU.},
	language = {en},
	number = {1},
	urldate = {2020-10-01},
	journal = {Scientific Reports},
	author = {Stimberg, Marcel and Goodman, Dan F. M. and Nowotny, Thomas},
	month = jan,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {410},
	file = {Full Text PDF:/home/seb/Zotero/storage/Y6N3YYFG/Stimberg et al. - 2020 - Brian2GeNN accelerating spiking neural network si.pdf:application/pdf;Snapshot:/home/seb/Zotero/storage/ESLBC6JF/s41598-019-54957-7.html:text/html}
}

@article{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the ﬁrst deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We ﬁnd that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	language = {en},
	urldate = {2020-10-09},
	journal = {arXiv:1312.5602 [cs]},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.5602},
	keywords = {Computer Science - Machine Learning},
	file = {Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:/home/seb/Zotero/storage/QJ7S2LTZ/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf}
}

@article{bojarski_end_2016,
	title = {End to {End} {Learning} for {Self}-{Driving} {Cars}},
	url = {http://arxiv.org/abs/1604.07316},
	abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
	urldate = {2020-10-09},
	journal = {arXiv:1604.07316 [cs]},
	author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.07316},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/seb/Zotero/storage/DSEWM3DU/Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars.pdf:application/pdf;arXiv.org Snapshot:/home/seb/Zotero/storage/647U8X58/1604.html:text/html}
}

@article{bojarski_explaining_2017,
	title = {Explaining {How} a {Deep} {Neural} {Network} {Trained} with {End}-to-{End} {Learning} {Steers} a {Car}},
	url = {http://arxiv.org/abs/1704.07911},
	abstract = {As part of a complete software stack for autonomous driving, NVIDIA has created a neural-network-based system, known as PilotNet, which outputs steering angles given images of the road ahead. PilotNet is trained using road images paired with the steering angles generated by a human driving a data-collection car. It derives the necessary domain knowledge by observing human drivers. This eliminates the need for human engineers to anticipate what is important in an image and foresee all the necessary rules for safe driving. Road tests demonstrated that PilotNet can successfully perform lane keeping in a wide variety of driving conditions, regardless of whether lane markings are present or not. The goal of the work described here is to explain what PilotNet learns and how it makes its decisions. To this end we developed a method for determining which elements in the road image most influence PilotNet's steering decision. Results show that PilotNet indeed learns to recognize relevant objects on the road. In addition to learning the obvious features such as lane markings, edges of roads, and other cars, PilotNet learns more subtle features that would be hard to anticipate and program by engineers, for example, bushes lining the edge of the road and atypical vehicle classes.},
	urldate = {2020-10-09},
	journal = {arXiv:1704.07911 [cs]},
	author = {Bojarski, Mariusz and Yeres, Philip and Choromanska, Anna and Choromanski, Krzysztof and Firner, Bernhard and Jackel, Lawrence and Muller, Urs},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.07911},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/seb/Zotero/storage/QC2PRD7I/Bojarski et al. - 2017 - Explaining How a Deep Neural Network Trained with .pdf:application/pdf;arXiv.org Snapshot:/home/seb/Zotero/storage/ZL28S94K/1704.html:text/html}
}

@inproceedings{david_deepchess_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeepChess}: {End}-to-{End} {Deep} {Neural} {Network} for {Automatic} {Learning} in {Chess}},
	isbn = {978-3-319-44781-0},
	shorttitle = {{DeepChess}},
	doi = {10.1007/978-3-319-44781-0_11},
	abstract = {We present an end-to-end learning method for chess, relying on deep neural networks. Without any a priori knowledge, in particular without any knowledge regarding the rules of chess, a deep neural network is trained using a combination of unsupervised pretraining and supervised training. The unsupervised training extracts high level features from a given position, and the supervised training learns to compare two chess positions and select the more favorable one. The training relies entirely on datasets of several million chess games, and no further domain specific knowledge is incorporated.The experiments show that the resulting neural network (referred to as DeepChess) is on a par with state-of-the-art chess playing programs, which have been developed through many years of manual feature selection and tuning. DeepChess is the first end-to-end machine learning-based method that results in a grandmaster-level chess playing performance.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2016},
	publisher = {Springer International Publishing},
	author = {David, Omid E. and Netanyahu, Nathan S. and Wolf, Lior},
	editor = {Villa, Alessandro E.P. and Masulli, Paolo and Pons Rivero, Antonio Javier},
	year = {2016},
	keywords = {Chess Game, Deep Belief Network, Deep Neural Network, High Level Feature, Supervise Training},
	pages = {88--96},
	file = {Submitted Version:/home/seb/Zotero/storage/PZMWMBIJ/David et al. - 2016 - DeepChess End-to-End Deep Neural Network for Auto.pdf:application/pdf}
}

@article{thrun_learning_1995,
	title = {Learning to {Play} the {Game} of {Chess}},
	abstract = {This paper presents NeuroChess, a program which learns to play chess from the final outcome of games. NeuroChess learns chess board evaluation functions, represented by artificial neural networks. It integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning. Performance results illustrate some of the strengths and weaknesses of this approach.},
	language = {en},
	journal = {Advances in neural information processing systems.},
	author = {Thrun, Sebastian},
	year = {1995},
	pages = {1069--1076},
	file = {Thrun - Learning to Play the Game of Chess.pdf:/home/seb/Zotero/storage/HS7DXS89/Thrun - Learning to Play the Game of Chess.pdf:application/pdf}
}

@article{silver_general_2018,
	title = {A general reinforcement learning algorithm that masters chess, shogi, and {Go} through self-play},
	volume = {362},
	copyright = {Copyright © 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/362/6419/1140},
	doi = {10.1126/science.aar6404},
	abstract = {One program to rule them all
Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.
Science, this issue p. 1140; see also pp. 1087 and 1118
The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.
AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each.
AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each.},
	language = {en},
	number = {6419},
	urldate = {2020-10-09},
	journal = {Science},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2018},
	pmid = {30523106},
	note = {Publisher: American Association for the Advancement of Science
Section: Report},
	pages = {1140--1144},
	file = {Full Text PDF:/home/seb/Zotero/storage/GPL9CW4K/Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:application/pdf;Snapshot:/home/seb/Zotero/storage/ABQXVIA3/1140.html:text/html}
}

@article{nageswaran_configurable_2009,
	series = {Advances in {Neural} {Networks} {Research}: {IJCNN2009}},
	title = {A configurable simulation environment for the efficient simulation of large-scale spiking neural networks on graphics processors},
	volume = {22},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608009001373},
	doi = {10.1016/j.neunet.2009.06.028},
	abstract = {Neural network simulators that take into account the spiking behavior of neurons are useful for studying brain mechanisms and for various neural engineering applications. Spiking Neural Network (SNN) simulators have been traditionally simulated on large-scale clusters, super-computers, or on dedicated hardware architectures. Alternatively, Compute Unified Device Architecture (CUDA) Graphics Processing Units (GPUs) can provide a low-cost, programmable, and high-performance computing platform for simulation of SNNs. In this paper we demonstrate an efficient, biologically realistic, large-scale SNN simulator that runs on a single GPU. The SNN model includes Izhikevich spiking neurons, detailed models of synaptic plasticity and variable axonal delay. We allow user-defined configuration of the GPU-SNN model by means of a high-level programming interface written in C++ but similar to the PyNN programming interface specification. PyNN is a common programming interface developed by the neuronal simulation community to allow a single script to run on various simulators. The GPU implementation (on NVIDIA GTX-280 with 1 GB of memory) is up to 26 times faster than a CPU version for the simulation of 100K neurons with 50 Million synaptic connections, firing at an average rate of 7 Hz. For simulation of 10 Million synaptic connections and 100K neurons, the GPU SNN model is only 1.5 times slower than real-time. Further, we present a collection of new techniques related to parallelism extraction, mapping of irregular communication, and network representation for effective simulation of SNNs on GPUs. The fidelity of the simulation results was validated on CPU simulations using firing rate, synaptic weight distribution, and inter-spike interval analysis. Our simulator is publicly available to the modeling community so that researchers will have easy access to large-scale SNN simulations.},
	language = {en},
	number = {5},
	urldate = {2020-10-09},
	journal = {Neural Networks},
	author = {Nageswaran, Jayram Moorkanikara and Dutt, Nikil and Krichmar, Jeffrey L. and Nicolau, Alex and Veidenbaum, Alexander V.},
	month = jul,
	year = {2009},
	keywords = {CUDA, Data parallelism, Graphics processor, Izhikevich spiking neuron, STDP},
	pages = {791--800},
	file = {ScienceDirect Full Text PDF:/home/seb/Zotero/storage/JFA22LJC/Nageswaran et al. - 2009 - A configurable simulation environment for the effi.pdf:application/pdf;ScienceDirect Snapshot:/home/seb/Zotero/storage/GEKCZ76E/S0893608009001373.html:text/html}
}
