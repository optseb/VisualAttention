% This is an example of using latex for a paper/report of specified
% size/layout. It's useful if you want to provide a PDF that looks
% like it was made in a normal word processor.

% While writing, don't stop for errors
\nonstopmode

% Use the article doc class, with an 11 pt basic font size
\documentclass[11pt, a4paper]{article}

% Makes the main font Nimbus Roman, a Times New Roman lookalike:
%\usepackage{mathptmx}% http://ctan.org/pkg/mathptmx
% OR use this for proper Times New Roman (from msttcorefonts package
% on Ubuntu). Use xelatex instead of pdflatex to compile:
\usepackage{fontspec}
\usepackage{xltxtra}
\usepackage{xunicode}
\defaultfontfeatures{Scale=MatchLowercase,Mapping=tex-text}
\setmainfont{Times New Roman}

% Set margins
\usepackage[margin=2.5cm]{geometry}

% Multilingual support
\usepackage[english]{babel}

% Nice mathematics
\usepackage{amsmath}

% Control over maketitle
\usepackage{titling}

% Section styling
\usepackage{titlesec}

% Syntax highlighted code listing
\usepackage{listings}

% Ability to use colour in text
\usepackage[usenames]{color}
% Colours for code
%\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codered}{rgb}{0.95,0.0,0.0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,1,1}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codered},
    keywordstyle=\color{codepurple},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codegreen},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}


% For the \degree symbol
\usepackage{gensymb}

% Allow includegraphics and nice wrapped figures
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[outercaption]{sidecap}

% Set formats using titlesec
\titleformat*{\section}{\bfseries\rmfamily}
\titleformat*{\subsection}{\bfseries\itshape\rmfamily}

% thetitle is the number of the section. This sets the distance from
% the number to the section text.
\titlelabel{\thetitle.\hskip0.3em\relax}

% Set title spacing with titlesec, too.  The first {1.0ex plus .2ex
% minus .7ex} sets the spacing above the section title. The second
% {-1.0ex plus 0.2ex} sets the spacing the section title to the
% paragraph.
\titlespacing{\section}{0pc}{1.0ex plus .2ex minus .7ex}{-1.1ex plus 0.2ex}

%% Trick to define a language alias and permit language = {en} in the .bib file.
% From: http://tex.stackexchange.com/questions/199254/babel-define-language-synonym
\usepackage{letltxmacro}
\LetLtxMacro{\ORIGselectlanguage}{\selectlanguage}
\makeatletter
\DeclareRobustCommand{\selectlanguage}[1]{%
  \@ifundefined{alias@\string#1}
    {\ORIGselectlanguage{#1}}
    {\begingroup\edef\x{\endgroup
       \noexpand\ORIGselectlanguage{\@nameuse{alias@#1}}}\x}%
}
\newcommand{\definelanguagealias}[2]{%
  \@namedef{alias@#1}{#2}%
}
\makeatother
\definelanguagealias{en}{english}
\definelanguagealias{eng}{english}
%% End language alias trick

%% Any aliases here
\newcommand{\myalias}{Some text or something}
% Emphasis and bold.
\newcommand{\e}{\emph}
\newcommand{\mycite}[1]{\cite{#1}}
%% END aliases

% Custom font defs
% fontsize is \fontsize{fontsize}{linespacesize}
\def\authorListFont{\fontsize{11}{11} }
\def\corrAuthorFont{\fontsize{10}{10} }
\def\affiliationListFont{\fontsize{11}{11}\itshape }
\def\titleFont{\fontsize{14}{11} \bfseries }
\def\textFont{\fontsize{11}{11} }
\def\sectionHdrFont{\fontsize{11}{11}\bfseries}
\def\bibFont{\fontsize{10}{10} }
\def\captionFont{\fontsize{10}{10} }

% Caption font size to be small.
\usepackage[font=small,labelfont=bf]{caption}

\def\firstAuthorLast{James {et~al.}}

% Affiliations
\def\Address{\\
\affiliationListFont 1 Department of Psychology,  The University of Sheffield, Sheffield, UK \\
}

% The Corresponding Author should be marked with an asterisk. Provide
% the exact contact address (this time including street name and city
% zip code) and email of the corresponding author
\def\corrAuthor{Seb James}
\def\corrAddress{Department of Psychology, The University of Sheffield,
  Western Bank, Sheffield, S10 2TP, UK}
\def\corrEmail{seb.james@sheffield.ac.uk}

% Figure out the font for the author list..
\def\Authors{\authorListFont Sebastian James\,$^{*}$,
 \Address \\
  \corrAuthorFont $^{*}$ Correspondence: \corrEmail}

% No page numbering please
\pagenumbering{gobble}

% A trick to get the bibliography to show up with 1. 2. etc in place
% of [1], [2] etc.:
\makeatletter
\renewcommand\@biblabel[1]{#1.}
\makeatother

% reduce separation between bibliography items if not using natbib:
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}

% Set correct font for bibliography (doesn't work yet)
%\renewcommand*{\bibfont}{\bibFont}

% No paragraph indenting to match the VPH format
\setlength{\parindent}{0pt}

% Skip a line after paragraphs
\setlength{\parskip}{0.5\baselineskip}
\onecolumn

% titling definitions
\pretitle{\begin{center}\titleFont}
\posttitle{\par\end{center}\vskip 0em}
\preauthor{ % Fonts are set within \Authors
        \vspace{-1.1cm} % Bring authors up towards title
        \begin{center}
        \begin{tabular}[t]{c}
}
\postauthor{\end{tabular}\par\end{center}}

% Define title, empty date and authors
\title {Using the GPU to compute connection functions in neurophysiologically
        realistic neural networks}
\date{} % No date please
\author{\Authors}

%% END OF PREAMBLE

\begin{document}

\setlength{\droptitle}{-1.8cm} % move the title up a suitable amount
\maketitle

\vspace{-1.8cm} % HACK bring the introduction up towards the title. It
                % would be better to do this with titling in \maketitle

\section{Introduction}

% Add your figure
%\begin{wrapfigure}{r}{0.4\textwidth}
%\includegraphics[width=0.4\textwidth]{./figures/somefigure.png}
%\caption{A wrapped figure example.}
%\vspace{-10pt}
%\label{model_overview}
%\end{wrapfigure}

%The enormous value of the games industry has ensured that development of the
%GPU has been sustained and prolonged, in contrast to other non-standard chips
%such as neuromorphic chips (cite) and asynchronous chips (cite. really?
%perhaps these are in use more than I think. ask Mike).

The graphical processing unit (GPU) is a specialised processer designed to
carry out thousands of parallel computations. The need to process graphical
scenes for computer games and movies has motivated sustained investment in the
development of these devices which have contributed to a huge growth in both
industries. A highly parallel processor is ideally suited for computer
graphics because generating the image for the monitor provides the perfect
parallelizable problem; it consists of millions of pixels, each of which must
have its 3 colours specified to form an image. For a 1080p monitor, that's 6
million parallel tasks. The beauty of the problem is that the final
integration of the
\emph{meaning} of these 6 million results is performed by the gamer's brain! In
contrast, the solution of most mathematical problems requires `the pixels to
talk to one another': the computational elements must transfer information in
order to deliver a final result or to update variables at every timestep of a
simulated system. Information transfer is a task to which the GPU is, \emph{by
design} less well suited.

In spite of this drawback, from about the mid-2000s researchers began to
explore the use of GPUs for general purpose computation, seeking to identify
those problems which would benefit most from the GPU's parallel computational
power.  Artificial neural networks (ANNs) have provided a well known success,
seeding an entire `deep learning' industry. Here, the GPU speed-up of the
back-propapagation of error makes possible the training of very large networks
that can solve impressively difficult problems such as driving a
car~\cite{bojarski_end_2016,bojarski_explaining_2017}, playing board games
such as chess~\cite{thrun_learning_1995,david_deepchess_2016} or Go~\cite{silver_general_2018} and console games such as
Pong~\cite{mnih_playing_2013}. Note that the GPU significantly accelerates
the \emph{training} of the network, rather the execution of the network
post-training.

Researchers in the field of computational neuroscience have also explored the
use of the GPU to simulate neurophysiologically realistic neural networks.
These generally involve a complex neuron model in which the cell's membrane
voltage and the release of numerous neurotransmitters may be modelled to
understand the network's behaviour. The benefit which the GPU can offer
computational neuroscience depends on the complexity of the neuron model and
the connectivity of the network; $N$ neurons in a network can be simulated in
parallel by $N$ processing threads for one timestep before their outputs must
be transferred according to the connectivity of the network. The GPU is
well-suited to the parallel execution of the neuron models, but not for the
transfer of signals, which requires that GPU threads coordinate memory
accesses. The best speed-ups reported involve simulation of complex,
multi-compartment models~\cite{stimberg_brian2genn_2020}. Results are less
favourable for networks involving the more parsimonious neuron models often
employed \emph{for their speed and efficiency} in computational neuroscience
studies \cite{nageswaran_configurable_2009}. Given the significant additional
complexity of GPU code development, adoption of GPU computation in
computational neuroscience studies is by no means ubiquitous.

However, there is one practical task in the development of neuroscience models
which \emph{is} amenable to GPU acceleration: the computation of connectivity
patterns. It is common to define the connection patterns between populations
in a biological neural network model according to hypothesis or on the basis
of experimental observations. The purpose of this letter is to claim that this
application of the GPU should not be neglected and to give a sample
implementation in python/Numba-cuda. The motivation for this work was the
construction, in
SpineCreator \cite{cope_spinecreator_2015,cope_spinecreator:_2016}, of a
visual attention model in which a number of neural populations serve as image
maps. Each population is formed into 150 $\times$ 150 grids. Thus each
population contains 22,500 neurons. Projections from one population to another
take the form of Gaussian projections [Fig]. These projections have the
strongest weights where the source neurons and the destination neurons are
closest; neurons in the centre of the source population project most strongly
to those in the destination population. The weights drop off as the
destination neurons become more distal from the source neurons, and below some
threshold, the weights are set to zero. Examples can be found
in \cite{james_integrating_2018}, Figs. 1C and 1D.

Relatively few neurons project from a given source neuron to the destination
population, but to actually determine which those are, it is necessary to
evaluate the connection between every source/destination pair. In our model,
this results in 22,500 $\times$ 22,500 $=$ 506,250,000 weight pairs. The
projections in the model are variations on this theme; some have widening
projections patterns; some have spatial offsets but the number of weight pairs
is always the same.

An example computation to be carried out is given below. This takes the
SpineCreator connectionFunc format; a defined API for a python function which
computes a weight table for the connection pattern. This connection function
computes a `widening Gaussian'; the projection pattern widens along one axis
of the (square) population, modelling the projection from the retina, with its
high acuity fovea, to destination tissue such as the superior colliculus. As
for all SpineCreator connection unctions, it passes an array of source neuron
coordinates (src), an array of destination neuron coordinates, and a number of
connection-specific parameters as arguments and expects a table of connections
to be returned. This is a reducing operation; $5\times10^8$ possible
connections are reduced down to a table consisting of perhaps a few hundred
thousand non-negligible weights.

\begin{lstlisting}[language=Python]
def connectionFunc(src,dst,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsd0p,offsd1r):
  # Function arguments:
  # src: an array of 3D coordinates of the locations of the source neurons
  # dst: array of coordinates of the destination neurons
  # sigma_m: The average width of the projection
  # E2: A parameter
  # fovshift: A foveal shift parameter
  # nfs: The neural field size parameter - its value is 150
  # W_cut: Minimum weight to be included in the output table
  # offsd0p and offsd1r: Spatial offsets for the projection
  import math
  # The starting magnification factor
  M_f_start = nfs/(E2*math.log((fovshift/(2*E2))+1))
  i_src = 0 # index into src
  out = [] # out will be filled with the output table and returned
  for s in src:
    i_dst = 0 # index into dst
    # Compute the location of s, this defines what sigma will be.
    # As r (as opp. to phi) increases, the sigma should increase.
    # M_f is the function of r, aka s[1].
    M_f = nfs/(E2*math.log(((1+s[1])/(2*E2))+1))
    # Set some of M_f to 1 to ensure the fan-out starts at around the edge
    # of the foveal region.
    if (1+s[1]) < fovshift:
      M_f = M_f_start
    # Find the effective width of the Gaussian at this location:
    _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0
    three_sigma = 3 * _sigma
    # For each source neuron, loop over all destinations:
    for d in dst:
      # in-xy-plane distance (ignore s[2]/d[2])
      xd = (s[0] - d[0] + offsd0p)
      yd = (s[1] - d[1] + offsd1r)
      if abs(xd) < three_sigma and abs(yd) < three_sigma:
        # Compute distance from s to d:
        dist = math.sqrt(math.pow(xd,2) + math.pow(yd,2))
        # From distance compute gaussian weight:
        gauss = math.exp(-0.5*math.pow(dist/_sigma,2))
        # Add the weight to the output table only if it exceed the threshold, W_cut:
        if gauss > W_cut:
          conn = (i_src,i_dst,0,gauss)
          out.append(conn)
      i_dst = i_dst + 1
    i_src = i_src + 1
  return out
# end connectionFunc
\end{lstlisting}

Execution of the listing above on a relatively modern CPU takes about four
minutes. If the modeller wishes to change any parameter in the connection
pattern it must be recomputed. The visual attention model which forms our
example has at least ten projections which must be computed and so changing a
parameter in the projections in the model could require nearly an hour of
compute time and makes the model exceedingly tedious to work with.

The majority of the time required for the computation above occurs in the
middle loop over the destination neurons. \footnote{In fact, the example above
halves the computation time by testing the distances and only carrying out the
weight calculation for `nearby' neurons.} This maps very well onto a GPU as
each computation is independent. However, at first sight, it would appear to
be necessary to transfer the results to memory which the CPU can access, then
carry out all 506,250,000 weight comparisons with \code{W\_cut} in order to
build the reduced table of connections, losing any performance gain provided
by the GPU! The solution is to make use of the \emph{parallel prefix scan}
algorithm.

\section*{Appendix}

GPU widening Gaussian code listing.

\lstinputlisting[language=Python]{../../connectionFuncs/offset_retgauss_gpu.py}

%\section*{Acknowledgements}
%Acknowledge funders.

\selectlanguage{English}
\bibliographystyle{abbrvnotitle}
% The bibliography NoTremor.bib is the one exported from Zotero.
\bibliography{GPU}

%%% Upload the *bib file along with the *tex file and PDF on
%%% submission if the bibliography is not in the main *tex file

\end{document}
