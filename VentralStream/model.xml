<?xml version="1.0" encoding="UTF-8"?>
<LL:SpineML xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.shef.ac.uk/SpineMLNetworkLayer" xmlns:LL="http://www.shef.ac.uk/SpineMLLowLevelNetworkLayer" xsi:schemaLocation="http://www.shef.ac.uk/SpineMLLowLevelNetworkLayer SpineMLLowLevelNetworkLayer.xsd http://www.shef.ac.uk/SpineMLNetworkLayer SpineMLNetworkLayer.xsd" name="Ventral Stream">
    <LL:Population>
        <LL:Annotation>
            <SpineCreator>
                <xPos value="-0.780396"/>
                <yPos value="-1.51214"/>
                <animSpeed value="0.2"/>
                <aspectRatio value="1.66667"/>
                <colour red="85" green="170" blue="0"/>
                <size value="1"/>
                <tag value="1"/>
                <x3D value="0"/>
                <y3D value="0"/>
                <z3D value="0"/>
                <is_visualised value="0"/>
            </SpineCreator>
        </LL:Annotation>
        <LL:Neuron name="World" size="22500" url="WorldToBrain.xml">
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </LL:Neuron>
        <Layout url="grid.xml" seed="123" minimum_distance="0">
            <Property name="numNeurons" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="rowlen" dimension="?">
                <FixedValue value="150"/>
            </Property>
            <Property name="x" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="z" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="t" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </Layout>
        <LL:Projection dst_population="V1_p_edges">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="-0.899696" y="-1.01214"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.975796" ypos="1.00376"/>
                            <C2 xpos="-2.3237" ypos="-0.838643"/>
                            <end xpos="-2.38359" ypos="1.08044"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=nfs      #LOC=1,1&#10;#PARNAME=sigma_g  #LOC=2,1&#10;#PARNAME=gain_g   #LOC=2,2&#10;#PARNAME=lambda_s #LOC=3,1&#10;#PARNAME=gain_s   #LOC=3,2&#10;#PARNAME=dir_s    #LOC=4,1&#10;#PARNAME=W_cut    #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# This implements a Gabor connection; a 2-D Gaussian multiplied by a&#10;# 1-D sine. Arguments are:&#10;#&#10;# sigma_g - sigma of the gaussian&#10;# gain_g - a gain for the gaussian&#10;# lambda_s - wavelength of sine&#10;# gain_s - gain of sine&#10;# dir_s - direction of (1-D) sine in degrees&#10;# W_cut - weight cut-off; weights below this considered to be 0&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,nfs,sigma_g,gain_g,lambda_s,gain_s,dir_s,W_cut,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy&#10;        # carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(int32,int32[:,:],int32[:,:],float32[:],float32, float32,float32,  float32, float32,float32)&quot;, nopython=True)&#10;    def dowork (nfs_sq,   d_src_ar,  d_dst_ar,  weight_ar, sigma_g, gain_g, lambda_s, gain_s,  dir_s,  W_cut):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        twopi = float32(6.283185307)&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;&#10;            # Avoid many tanh, sin, cos, pow and exp computations for well-separated neurons:&#10;            xdist = d_src_ar[i_src,0] - d_dst_ar[i_dst,0]&#10;            ydist = d_src_ar[i_src,1] - d_dst_ar[i_dst,1]&#10;&#10;            # Testing the region of interest gives a slight speed up&#10;            # (7 s vs 8 s) at a cost of having one extra parameter to&#10;            # set.&#10;            # zdist = d_src_ar[i_src,2] - d_dst_ar[i_dst,2] # ignore z component for now&#10;            dist = math.sqrt(math.pow(xdist,2) + math.pow(ydist,2)) # + math.pow(zdist,2))&#10;            # Direction from source to dest&#10;            top = float32(d_dst_ar[i_dst,1]-d_src_ar[i_src,1])&#10;            bot = float32(d_dst_ar[i_dst,0]-d_src_ar[i_src,0])&#10;            dir_d = math.atan2 (top, bot)&#10;            # Find the projection of the source-&gt;dest direction onto the sine wave direction. Call this distance dprime.&#10;            dprime = dist*math.cos(dir_d + twopi - ((dir_s*twopi)/float32(360)));&#10;            # Use dprime to figure out what the sine weight is.&#10;            sine_weight = gain_s*math.sin(dprime*twopi/lambda_s);&#10;            gauss_weight = gain_g*math.exp(-0.5*math.pow(dist/sigma_g,2))&#10;            combined_weight = sine_weight * gauss_weight;&#10;&#10;            if abs(combined_weight) &gt; W_cut:&#10;                tmp_w[offsidx] = float32(combined_weight)&#10;                tmp_w[offsidx+1] = float32(i_src)&#10;                tmp_w[offsidx+2] = float32(i_dst)&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    weight_ar[weight_idx] = theweight&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    time_start = int(round(time.time() * 1000))&#10;&#10;    # Do the work of computing the connections:&#10;    dowork[blockspergrid, threadsperblock](nfs_sq, d_src_ar, d_dst_ar, d_weight_ar, sigma_g, gain_g, lambda_s, gain_s, dir_s, W_cut)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="gabor_gpu" nfs="150" sigma_g="1" gain_g="1" lambda_s="8" gain_s="1" dir_s="0" W_cut="0.001" max_weights="2e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_World_to_V1_p_edges_syn0.bin" num_connections="417654" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="0"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="World to V1_p_edges Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile0.bin" num_elements="417654"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="World to V1_p_edges Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V1_r_edges">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="-0.578796" y="-1.01214"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.573696" ypos="0.957067"/>
                            <C2 xpos="1.0793" ypos="-0.577014"/>
                            <end xpos="0.983103" ypos="1.06237"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=nfs      #LOC=1,1&#10;#PARNAME=sigma_g  #LOC=2,1&#10;#PARNAME=gain_g   #LOC=2,2&#10;#PARNAME=lambda_s #LOC=3,1&#10;#PARNAME=gain_s   #LOC=3,2&#10;#PARNAME=dir_s    #LOC=4,1&#10;#PARNAME=W_cut    #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# This implements a Gabor connection; a 2-D Gaussian multiplied by a&#10;# 1-D sine. Arguments are:&#10;#&#10;# sigma_g - sigma of the gaussian&#10;# gain_g - a gain for the gaussian&#10;# lambda_s - wavelength of sine&#10;# gain_s - gain of sine&#10;# dir_s - direction of (1-D) sine in degrees&#10;# W_cut - weight cut-off; weights below this considered to be 0&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,nfs,sigma_g,gain_g,lambda_s,gain_s,dir_s,W_cut,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy&#10;        # carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(int32,int32[:,:],int32[:,:],float32[:],float32, float32,float32,  float32, float32,float32)&quot;, nopython=True)&#10;    def dowork (nfs_sq,   d_src_ar,  d_dst_ar,  weight_ar, sigma_g, gain_g, lambda_s, gain_s,  dir_s,  W_cut):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        twopi = float32(6.283185307)&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;&#10;            # Avoid many tanh, sin, cos, pow and exp computations for well-separated neurons:&#10;            xdist = d_src_ar[i_src,0] - d_dst_ar[i_dst,0]&#10;            ydist = d_src_ar[i_src,1] - d_dst_ar[i_dst,1]&#10;&#10;            # Testing the region of interest gives a slight speed up&#10;            # (7 s vs 8 s) at a cost of having one extra parameter to&#10;            # set.&#10;            # zdist = d_src_ar[i_src,2] - d_dst_ar[i_dst,2] # ignore z component for now&#10;            dist = math.sqrt(math.pow(xdist,2) + math.pow(ydist,2)) # + math.pow(zdist,2))&#10;            # Direction from source to dest&#10;            top = float32(d_dst_ar[i_dst,1]-d_src_ar[i_src,1])&#10;            bot = float32(d_dst_ar[i_dst,0]-d_src_ar[i_src,0])&#10;            dir_d = math.atan2 (top, bot)&#10;            # Find the projection of the source-&gt;dest direction onto the sine wave direction. Call this distance dprime.&#10;            dprime = dist*math.cos(dir_d + twopi - ((dir_s*twopi)/float32(360)));&#10;            # Use dprime to figure out what the sine weight is.&#10;            sine_weight = gain_s*math.sin(dprime*twopi/lambda_s);&#10;            gauss_weight = gain_g*math.exp(-0.5*math.pow(dist/sigma_g,2))&#10;            combined_weight = sine_weight * gauss_weight;&#10;&#10;            if abs(combined_weight) &gt; W_cut:&#10;                tmp_w[offsidx] = float32(combined_weight)&#10;                tmp_w[offsidx+1] = float32(i_src)&#10;                tmp_w[offsidx+2] = float32(i_dst)&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    weight_ar[weight_idx] = theweight&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    time_start = int(round(time.time() * 1000))&#10;&#10;    # Do the work of computing the connections:&#10;    dowork[blockspergrid, threadsperblock](nfs_sq, d_src_ar, d_dst_ar, d_weight_ar, sigma_g, gain_g, lambda_s, gain_s, dir_s, W_cut)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="gabor_gpu" nfs="150" sigma_g="1" gain_g="1" lambda_s="8" gain_s="1" dir_s="90" W_cut="0.001" max_weights="2e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_World_to_V1_r_edges_syn0.bin" num_connections="417654" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="0"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="World to V1_r_edges Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile3.bin" num_elements="417654"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="World to V1_r_edges Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
    </LL:Population>
    <LL:Population>
        <LL:Annotation>
            <SpineCreator>
                <xPos value="-2.3599"/>
                <yPos value="1.58044"/>
                <animSpeed value="0.2"/>
                <aspectRatio value="1.66667"/>
                <colour red="255" green="0" blue="0"/>
                <size value="1"/>
                <tag value="2"/>
                <x3D value="0"/>
                <y3D value="0"/>
                <z3D value="11"/>
                <is_visualised value="0"/>
            </SpineCreator>
        </LL:Annotation>
        <LL:Neuron name="V1_p_edges" size="22500" url="LINtanh.xml">
            <Property name="tau" dimension="?">
                <FixedValue value="10"/>
            </Property>
            <Property name="noise" dimension="?">
                <FixedValue value="0.01"/>
            </Property>
            <Property name="a" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </LL:Neuron>
        <Layout url="grid.xml" seed="123" minimum_distance="0">
            <Property name="numNeurons" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="rowlen" dimension="?">
                <FixedValue value="150"/>
            </Property>
            <Property name="x" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="z" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="t" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </Layout>
        <LL:Projection dst_population="V2_p_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="-2.60073" y="2.08044"/>
                    <curves>
                        <curve>
                            <C1 xpos="-2.58567" ypos="4.21171"/>
                            <C2 xpos="-5.82938" ypos="3.82888"/>
                            <end xpos="-5.81622" ypos="6.12779"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=sigma_m        #LOC=1,1&#10;#PARNAME=E2             #LOC=1,2&#10;#PARNAME=sigma_0        #LOC=2,1&#10;#PARNAME=fovshift       #LOC=2,2&#10;#PARNAME=nfs            #LOC=3,1&#10;#PARNAME=W_cut          #LOC=3,2&#10;#PARNAME=offsetd0p      #LOC=4,1&#10;#PARNAME=offsetd1r      #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# Compute a widening Gaussian connection function for a retinotopic&#10;# space, to maintain a constant Gaussian width in Cartesian&#10;# space. This is much like the WideningGaussian connection function,&#10;# but with a configurable neural field size width (W_nfs).&#10;#&#10;# This version incorporates an offset for dstloc[0] and dstloc[1] to&#10;# shift the Gaussian projection by a desired amount.&#10;#&#10;# ... AND it does so to dual Gaussians, offset by +/-offsetd0p and +/-offsetd1r&#10;#&#10;# offsetd0p is the +/- phi direction&#10;# offsetd1r is in the +/- r direction&#10;#&#10;# Considering the r direction, r_d^max, the destination r value for&#10;# max connection strength for a given r_s is given by&#10;#&#10;# r_d^max = r_s + offsetd1r&#10;#&#10;# Thus for positive offsetd1r, &quot;connections are stronger in the&#10;# positive r direction away from the source&quot;.&#10;#&#10;# For positive offsetd0p, &quot;connections are stronger in the&#10;# positive p direction away from the source&quot;.&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsetd0p,offsetd1r,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Ensure these are fixed to being ints&#10;    _offsetd0p = int(offsetd0p)&#10;    _offsetd1r = int(offsetd1r)&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # To pad the arrays out to exact number of blocks&#10;        # arraysz1 = nfs_sq&#10;        #if arraysz%threadsperblock &gt; 0:&#10;        #    amod = arraysz%threadsperblock&#10;        #    print ('--\namod: {0}'.format(amod))&#10;        #    print ('arraysz: {0}'.format(arraysz))&#10;        #    print ('threadsperblock: {0}'.format(threadsperblock))&#10;        #    arrayszplus = arraysz + threadsperblock - amod&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;        #else:&#10;        #    print ('Set arrayszplus to arraysz1...')&#10;        #    arrayszplus = arraysz&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        #print ('j starts at {0}'.format(j))&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(float32,int32,  int32[:,:],int32[:,:],float32[:],float32, float32,float32, float32,  float32,float32, int32,     int32)&quot;)&#10;    def dowork (M_f_start,  nfs_sq, d_src_ar,  d_dst_ar,  weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,    W_cut,   offsetd0p, offsetd1r):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        ##print ('i_src: {0}, i_dst: {1}'.format(i_src, i_dst))&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            ##print ('cuda.threadIdx.y: {0} cuda.blockDim.x: {1} cuda.threadIdx.x: {2}'.format(cuda.threadIdx.y, cuda.blockDim.x, cuda.threadIdx.x))&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            ##print('myidx: {0}, shifted: {1}'.format(myidx, offsidx))&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;            ##print ('After syncthreads: tmp_w[0]:{0} [1]:{1} [2]:{2}'.format(tmp_w[0], tmp_w[1], tmp_w[2]))&#10;&#10;            # Compute the location of d_src_ar, this defines what sigma will be. As r (as opp. to phi) increases, the sigma should increase.&#10;            M_f = float32(nfs)/(E2*math.log(((1+d_src_ar[i_src,1])/(2*E2))+1))&#10;&#10;            # Set some of M_f to 1 to ensure the fan-out starts at around the edge of the foveal region.&#10;            if (1+d_src_ar[i_src,1]) &lt; fovshift:&#10;                M_f = M_f_start&#10;&#10;            # Compute modified sigma and 3 times this value&#10;            _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0&#10;            #three_sigma = float32(3.0) * _sigma&#10;&#10;            # in-xy-plane distance (ignore d_src_ar[2]/dstdoc[2])&#10;            xd = (d_src_ar[i_src,0] - d_dst_ar[i_dst,0])# + offsetd0p)&#10;            yd = (d_src_ar[i_src,1] - d_dst_ar[i_dst,1])# + offsetd1r)&#10;&#10;            dist1 = math.sqrt(math.pow((xd+offsetd0p),2) + math.pow((yd+offsetd1r),2))&#10;            dist2 = math.sqrt(math.pow((xd-offsetd0p),2) + math.pow((yd-offsetd1r),2))&#10;            gauss1 = math.exp(-0.5*math.pow(dist1/_sigma,2))&#10;            gauss2 = math.exp(-0.5*math.pow(dist2/_sigma,2))&#10;&#10;            if gauss1 &gt; W_cut:&#10;                tmp_w[offsidx] = float32(gauss1)&#10;                tmp_w[offsidx+1] = float32(i_src)&#10;                tmp_w[offsidx+2] = float32(i_dst)&#10;            elif gauss2 &gt; W_cut:&#10;                tmp_w[offsidx] = float32(gauss2)&#10;                tmp_w[offsidx+1] = float32(i_src)&#10;                tmp_w[offsidx+2] = float32(i_dst)&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    weight_ar[weight_idx] = theweight&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    M_f_start=nfs/(E2*math.log((fovshift/(2*E2))+1))&#10;    print('M_f_start: {0:f}'.format(M_f_start))&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    print('Numpy arrays...')&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    print('cuda.to_device...')&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    #                                &quot;void(float32,   int32,  int32[:,:], int32[:,:], float32[:],  float32, float32,float32, float32,  float32, float32,int32,     int32)&#10;    time_start = int(round(time.time() * 1000))&#10;    dowork[blockspergrid, threadsperblock](M_f_start, nfs_sq, d_src_ar,   d_dst_ar,   d_weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,     W_cut,  _offsetd0p, _offsetd1r)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="offsetdual_retgauss_gpu" sigma_m="30" E2="2.5" sigma_0="0.3" fovshift="1" nfs="150" W_cut="0.001" offsetd0p="4" offsetd1r="0" max_n_weights="3e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_V1_p_edges_to_V2_p_lines_syn0.bin" num_connections="3878176" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="V1_p_edges to V2_p_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile2.bin" num_elements="3878176"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_p_edges to V2_p_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="-2.1018" y="2.08044"/>
                    <curves>
                        <curve>
                            <C1 xpos="-1.48893" ypos="4.31083"/>
                            <C2 xpos="-2.51705" ypos="3.06797"/>
                            <end xpos="-2.51417" ypos="5.14418"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=sigma_m        #LOC=1,1&#10;#PARNAME=E2             #LOC=1,2&#10;#PARNAME=sigma_0        #LOC=2,1&#10;#PARNAME=fovshift       #LOC=2,2&#10;#PARNAME=nfs            #LOC=3,1&#10;#PARNAME=W_cut          #LOC=3,2&#10;#PARNAME=offsetd0p      #LOC=4,1&#10;#PARNAME=offsetd1r      #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# Compute a widening Gaussian connection function for a retinotopic&#10;# space, to maintain a constant Gaussian width in Cartesian&#10;# space. This is much like the WideningGaussian connection function,&#10;# but with a configurable neural field size width (W_nfs).&#10;#&#10;# This version incorporates an offset for dstloc[0] and dstloc[1] to&#10;# shift the Gaussian projection by a desired amount.&#10;#&#10;# Considering the r direction, r_d^max, the destination r value for&#10;# max connection strength for a given r_s is given by&#10;#&#10;# r_d^max = r_s + offsetd1r&#10;#&#10;# Thus for positive offsetd1r, &quot;connections are stronger in the&#10;# positive r direction away from the source&quot;.&#10;#&#10;# For positive offsetd0p, &quot;connections are stronger in the&#10;# positive p direction away from the source&quot;.&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsetd0p,offsetd1r,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Ensure these are fixed to being ints&#10;    _offsetd0p = int(offsetd0p)&#10;    _offsetd1r = int(offsetd1r)&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # To pad the arrays out to exact number of blocks&#10;        # arraysz1 = nfs_sq&#10;        #if arraysz%threadsperblock &gt; 0:&#10;        #    amod = arraysz%threadsperblock&#10;        #    print ('--\namod: {0}'.format(amod))&#10;        #    print ('arraysz: {0}'.format(arraysz))&#10;        #    print ('threadsperblock: {0}'.format(threadsperblock))&#10;        #    arrayszplus = arraysz + threadsperblock - amod&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;        #else:&#10;        #    print ('Set arrayszplus to arraysz1...')&#10;        #    arrayszplus = arraysz&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        #print ('j starts at {0}'.format(j))&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(float32,int32,  int32[:,:],int32[:,:],float32[:],float32, float32,float32, float32,  float32,float32, int32,     int32)&quot;)&#10;    def dowork (M_f_start,  nfs_sq, d_src_ar,  d_dst_ar,  weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,    W_cut,   offsetd0p, offsetd1r):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        ##print ('i_src: {0}, i_dst: {1}'.format(i_src, i_dst))&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            ##print ('cuda.threadIdx.y: {0} cuda.blockDim.x: {1} cuda.threadIdx.x: {2}'.format(cuda.threadIdx.y, cuda.blockDim.x, cuda.threadIdx.x))&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            ##print('myidx: {0}, shifted: {1}'.format(myidx, offsidx))&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;            ##print ('After syncthreads: tmp_w[0]:{0} [1]:{1} [2]:{2}'.format(tmp_w[0], tmp_w[1], tmp_w[2]))&#10;&#10;            # Compute the location of d_src_ar, this defines what sigma will be. As r (as opp. to phi) increases, the sigma should increase.&#10;            M_f = float32(nfs)/(E2*math.log(((1+d_src_ar[i_src,1])/(2*E2))+1))&#10;&#10;            # Set some of M_f to 1 to ensure the fan-out starts at around the edge of the foveal region.&#10;            if (1+d_src_ar[i_src,1]) &lt; fovshift:&#10;                ##print ('reset M_f to M_f_start because 1+d_src_ar[i_src={2},1] (1+{0:f}) &lt; fovshift ({1:f})'.format(d_src_ar[i_src,1], fovshift, i_src))&#10;                M_f = M_f_start&#10;&#10;            # Compute modified sigma and 3 times this value&#10;            _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0 # as function of r, aka d_src_ar[1]. M_f is the function of r.&#10;            three_sigma = float32(3.0) * _sigma&#10;            ##print ('i_src: {4} i_dst: {5}. d_src_ar: ({0:f},{1:f}) d_dst_ar: ({2:f},{3:f})'.format(d_src_ar[i_src,0], d_src_ar[i_src,1], d_dst_ar[i_dst,0], d_dst_ar[i_dst,1], i_src, i_dst))&#10;            ##if M_f != M_f_start:&#10;                ##print ('sigma_m: {0:f}, M_f:{1:f}. M_f_start:{2:f}. sigma_0: {3:f} _sigma: {4:f} three_sigma: {5:f}'.format(sigma_m, M_f, M_f_start, sigma_0, _sigma, three_sigma))&#10;&#10;            # in-xy-plane distance (ignore d_src_ar[2]/dstdoc[2])&#10;            xd = (d_src_ar[i_src,0] - d_dst_ar[i_dst,0] + offsetd0p)&#10;            yd = (d_src_ar[i_src,1] - d_dst_ar[i_dst,1] + offsetd1r)&#10;            if abs(xd) &lt; three_sigma and abs(yd) &lt; three_sigma:&#10;                ##print('(abs(xd) abs({0:f}) &lt; three_sigma {1:f} and abs(yd) abs({2:f}) &lt; three_sigma)'.format(xd, yd, three_sigma))&#10;                dist = math.sqrt(math.pow(xd,2) + math.pow(yd,2))&#10;                gauss = math.exp(-0.5*math.pow(dist/_sigma,2))&#10;                #gauss = float32(d_src_ar[i_dst,1])&#10;                ##print ('i_src: {0} i_dst: {1} gauss: {2}'.format(i_src, i_dst, gauss))&#10;                if gauss &gt; W_cut:&#10;                    # Write result into weight_ar&#10;                    ##print('gauss {0:f} &gt; W_cut {1:f} - WRITE RESULTS.'.format(gauss, W_cut))&#10;                    tmp_w[offsidx] = float32(gauss)&#10;                    tmp_w[offsidx+1] = float32(i_src)&#10;                    tmp_w[offsidx+2] = float32(i_dst)&#10;                    ##print('tmp_w[{0}] = {1:f}, tmp_w[{2}] = {3:f}, tmp_w[{4}] = {5:f}'.format(offsidx, tmp_w[offsidx], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2]))&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;            ##print ('After set tmp_w (myidx={6}): tmp_w[{0}]:{1:f} [{2}]:{3:f} [{4}]:{5:f}'.format(offsidx, tmp_w[offsidx+0], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2], myidx))&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;                ##print ('tpb = {0}. tmp_w[0]:{1} [1]:{2} [2]:{3} [3]:{4}'.format(tpb, tmp_w[0], tmp_w[1], tmp_w[2], tmp_w[3]))&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    ##print('myidx: {0}, idx: {1}, shifted: {2}'.format(myidx, idx, offsidx2))&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    #if gt(theweight, W_cut): # Testing makes no difference to speed&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    ##if (theweight &gt; 0.0):&#10;                        ##print ('Set weight_ar[{0}]={1}'.format(weight_idx, theweight))&#10;                    weight_ar[weight_idx] = theweight&#10;                    ##if weight_idx &gt; 0:&#10;                        ##print ('weight_ar[{0}] = {1:f}'.format(weight_idx, theweight))&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    M_f_start=nfs/(E2*math.log((fovshift/(2*E2))+1))&#10;    print('M_f_start: {0:f}'.format(M_f_start))&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    print('Numpy arrays...')&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    print('cuda.to_device...')&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    #                                &quot;void(float32,   int32,  int32[:,:], int32[:,:], float32[:],  float32, float32,float32, float32,  float32, float32,int32,     int32)&#10;    time_start = int(round(time.time() * 1000))&#10;    dowork[blockspergrid, threadsperblock](M_f_start, nfs_sq, d_src_ar,   d_dst_ar,   d_weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,     W_cut,  _offsetd0p, _offsetd1r)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="offset_retgauss_gpu" sigma_m="15" E2="2.5" sigma_0="0.3" fovshift="1" nfs="150" W_cut="0.001" offsetd0p="4" offsetd1r="0" max_n_weights="2e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_V1_p_edges_to_V2_pPp_rPr_syn0.bin" num_connections="650868" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="V1_p_edges to V2_pPp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile5.bin" num_elements="650868"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_p_edges to V2_pPp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V1_r_edges">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-1.52656" y="1.85941"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.620933" ypos="2.16257"/>
                            <C2 xpos="-0.469916" ypos="1.97673"/>
                            <end xpos="0.100069" ypos="1.80843"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V1_p_edges to V1_r_edges Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_p_edges to V1_r_edges Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="-2.08315" y="2.08044"/>
                    <curves>
                        <curve>
                            <C1 xpos="0.0337563" ypos="7.89597"/>
                            <C2 xpos="-2.22832" ypos="5.60847"/>
                            <end xpos="-2.27773" ypos="7.25032"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=sigma_m        #LOC=1,1&#10;#PARNAME=E2             #LOC=1,2&#10;#PARNAME=sigma_0        #LOC=2,1&#10;#PARNAME=fovshift       #LOC=2,2&#10;#PARNAME=nfs            #LOC=3,1&#10;#PARNAME=W_cut          #LOC=3,2&#10;#PARNAME=offsetd0p      #LOC=4,1&#10;#PARNAME=offsetd1r      #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# Compute a widening Gaussian connection function for a retinotopic&#10;# space, to maintain a constant Gaussian width in Cartesian&#10;# space. This is much like the WideningGaussian connection function,&#10;# but with a configurable neural field size width (W_nfs).&#10;#&#10;# This version incorporates an offset for dstloc[0] and dstloc[1] to&#10;# shift the Gaussian projection by a desired amount.&#10;#&#10;# Considering the r direction, r_d^max, the destination r value for&#10;# max connection strength for a given r_s is given by&#10;#&#10;# r_d^max = r_s + offsetd1r&#10;#&#10;# Thus for positive offsetd1r, &quot;connections are stronger in the&#10;# positive r direction away from the source&quot;.&#10;#&#10;# For positive offsetd0p, &quot;connections are stronger in the&#10;# positive p direction away from the source&quot;.&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsetd0p,offsetd1r,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Ensure these are fixed to being ints&#10;    _offsetd0p = int(offsetd0p)&#10;    _offsetd1r = int(offsetd1r)&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # To pad the arrays out to exact number of blocks&#10;        # arraysz1 = nfs_sq&#10;        #if arraysz%threadsperblock &gt; 0:&#10;        #    amod = arraysz%threadsperblock&#10;        #    print ('--\namod: {0}'.format(amod))&#10;        #    print ('arraysz: {0}'.format(arraysz))&#10;        #    print ('threadsperblock: {0}'.format(threadsperblock))&#10;        #    arrayszplus = arraysz + threadsperblock - amod&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;        #else:&#10;        #    print ('Set arrayszplus to arraysz1...')&#10;        #    arrayszplus = arraysz&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        #print ('j starts at {0}'.format(j))&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(float32,int32,  int32[:,:],int32[:,:],float32[:],float32, float32,float32, float32,  float32,float32, int32,     int32)&quot;)&#10;    def dowork (M_f_start,  nfs_sq, d_src_ar,  d_dst_ar,  weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,    W_cut,   offsetd0p, offsetd1r):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        ##print ('i_src: {0}, i_dst: {1}'.format(i_src, i_dst))&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            ##print ('cuda.threadIdx.y: {0} cuda.blockDim.x: {1} cuda.threadIdx.x: {2}'.format(cuda.threadIdx.y, cuda.blockDim.x, cuda.threadIdx.x))&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            ##print('myidx: {0}, shifted: {1}'.format(myidx, offsidx))&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;            ##print ('After syncthreads: tmp_w[0]:{0} [1]:{1} [2]:{2}'.format(tmp_w[0], tmp_w[1], tmp_w[2]))&#10;&#10;            # Compute the location of d_src_ar, this defines what sigma will be. As r (as opp. to phi) increases, the sigma should increase.&#10;            M_f = float32(nfs)/(E2*math.log(((1+d_src_ar[i_src,1])/(2*E2))+1))&#10;&#10;            # Set some of M_f to 1 to ensure the fan-out starts at around the edge of the foveal region.&#10;            if (1+d_src_ar[i_src,1]) &lt; fovshift:&#10;                ##print ('reset M_f to M_f_start because 1+d_src_ar[i_src={2},1] (1+{0:f}) &lt; fovshift ({1:f})'.format(d_src_ar[i_src,1], fovshift, i_src))&#10;                M_f = M_f_start&#10;&#10;            # Compute modified sigma and 3 times this value&#10;            _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0 # as function of r, aka d_src_ar[1]. M_f is the function of r.&#10;            three_sigma = float32(3.0) * _sigma&#10;            ##print ('i_src: {4} i_dst: {5}. d_src_ar: ({0:f},{1:f}) d_dst_ar: ({2:f},{3:f})'.format(d_src_ar[i_src,0], d_src_ar[i_src,1], d_dst_ar[i_dst,0], d_dst_ar[i_dst,1], i_src, i_dst))&#10;            ##if M_f != M_f_start:&#10;                ##print ('sigma_m: {0:f}, M_f:{1:f}. M_f_start:{2:f}. sigma_0: {3:f} _sigma: {4:f} three_sigma: {5:f}'.format(sigma_m, M_f, M_f_start, sigma_0, _sigma, three_sigma))&#10;&#10;            # in-xy-plane distance (ignore d_src_ar[2]/dstdoc[2])&#10;            xd = (d_src_ar[i_src,0] - d_dst_ar[i_dst,0] + offsetd0p)&#10;            yd = (d_src_ar[i_src,1] - d_dst_ar[i_dst,1] + offsetd1r)&#10;            if abs(xd) &lt; three_sigma and abs(yd) &lt; three_sigma:&#10;                ##print('(abs(xd) abs({0:f}) &lt; three_sigma {1:f} and abs(yd) abs({2:f}) &lt; three_sigma)'.format(xd, yd, three_sigma))&#10;                dist = math.sqrt(math.pow(xd,2) + math.pow(yd,2))&#10;                gauss = math.exp(-0.5*math.pow(dist/_sigma,2))&#10;                #gauss = float32(d_src_ar[i_dst,1])&#10;                ##print ('i_src: {0} i_dst: {1} gauss: {2}'.format(i_src, i_dst, gauss))&#10;                if gauss &gt; W_cut:&#10;                    # Write result into weight_ar&#10;                    ##print('gauss {0:f} &gt; W_cut {1:f} - WRITE RESULTS.'.format(gauss, W_cut))&#10;                    tmp_w[offsidx] = float32(gauss)&#10;                    tmp_w[offsidx+1] = float32(i_src)&#10;                    tmp_w[offsidx+2] = float32(i_dst)&#10;                    ##print('tmp_w[{0}] = {1:f}, tmp_w[{2}] = {3:f}, tmp_w[{4}] = {5:f}'.format(offsidx, tmp_w[offsidx], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2]))&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;            ##print ('After set tmp_w (myidx={6}): tmp_w[{0}]:{1:f} [{2}]:{3:f} [{4}]:{5:f}'.format(offsidx, tmp_w[offsidx+0], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2], myidx))&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;                ##print ('tpb = {0}. tmp_w[0]:{1} [1]:{2} [2]:{3} [3]:{4}'.format(tpb, tmp_w[0], tmp_w[1], tmp_w[2], tmp_w[3]))&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    ##print('myidx: {0}, idx: {1}, shifted: {2}'.format(myidx, idx, offsidx2))&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    #if gt(theweight, W_cut): # Testing makes no difference to speed&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    ##if (theweight &gt; 0.0):&#10;                        ##print ('Set weight_ar[{0}]={1}'.format(weight_idx, theweight))&#10;                    weight_ar[weight_idx] = theweight&#10;                    ##if weight_idx &gt; 0:&#10;                        ##print ('weight_ar[{0}] = {1:f}'.format(weight_idx, theweight))&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    M_f_start=nfs/(E2*math.log((fovshift/(2*E2))+1))&#10;    print('M_f_start: {0:f}'.format(M_f_start))&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    print('Numpy arrays...')&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    print('cuda.to_device...')&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    #                                &quot;void(float32,   int32,  int32[:,:], int32[:,:], float32[:],  float32, float32,float32, float32,  float32, float32,int32,     int32)&#10;    time_start = int(round(time.time() * 1000))&#10;    dowork[blockspergrid, threadsperblock](M_f_start, nfs_sq, d_src_ar,   d_dst_ar,   d_weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,     W_cut,  _offsetd0p, _offsetd1r)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="offset_retgauss_gpu" sigma_m="15" E2="2.5" sigma_0="0.3" fovshift="1" nfs="150" W_cut="0.001" offsetd0p="4" offsetd1r="0" max_n_weights="2e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_V1_p_edges_to_V2_pPp_rMr_syn0.bin" num_connections="650868" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="V1_p_edges to V2_pPp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile1.bin" num_elements="650868"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_p_edges to V2_pPp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="-2.075" y="2.08044"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.828692" ypos="4.71431"/>
                            <C2 xpos="0.357621" ypos="5.45928"/>
                            <end xpos="1.51314" ypos="7.14492"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=sigma_m        #LOC=1,1&#10;#PARNAME=E2             #LOC=1,2&#10;#PARNAME=sigma_0        #LOC=2,1&#10;#PARNAME=fovshift       #LOC=2,2&#10;#PARNAME=nfs            #LOC=3,1&#10;#PARNAME=W_cut          #LOC=3,2&#10;#PARNAME=offsetd0p      #LOC=4,1&#10;#PARNAME=offsetd1r      #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# Compute a widening Gaussian connection function for a retinotopic&#10;# space, to maintain a constant Gaussian width in Cartesian&#10;# space. This is much like the WideningGaussian connection function,&#10;# but with a configurable neural field size width (W_nfs).&#10;#&#10;# This version incorporates an offset for dstloc[0] and dstloc[1] to&#10;# shift the Gaussian projection by a desired amount.&#10;#&#10;# Considering the r direction, r_d^max, the destination r value for&#10;# max connection strength for a given r_s is given by&#10;#&#10;# r_d^max = r_s + offsetd1r&#10;#&#10;# Thus for positive offsetd1r, &quot;connections are stronger in the&#10;# positive r direction away from the source&quot;.&#10;#&#10;# For positive offsetd0p, &quot;connections are stronger in the&#10;# positive p direction away from the source&quot;.&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsetd0p,offsetd1r,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Ensure these are fixed to being ints&#10;    _offsetd0p = int(offsetd0p)&#10;    _offsetd1r = int(offsetd1r)&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # To pad the arrays out to exact number of blocks&#10;        # arraysz1 = nfs_sq&#10;        #if arraysz%threadsperblock &gt; 0:&#10;        #    amod = arraysz%threadsperblock&#10;        #    print ('--\namod: {0}'.format(amod))&#10;        #    print ('arraysz: {0}'.format(arraysz))&#10;        #    print ('threadsperblock: {0}'.format(threadsperblock))&#10;        #    arrayszplus = arraysz + threadsperblock - amod&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;        #else:&#10;        #    print ('Set arrayszplus to arraysz1...')&#10;        #    arrayszplus = arraysz&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        #print ('j starts at {0}'.format(j))&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(float32,int32,  int32[:,:],int32[:,:],float32[:],float32, float32,float32, float32,  float32,float32, int32,     int32)&quot;)&#10;    def dowork (M_f_start,  nfs_sq, d_src_ar,  d_dst_ar,  weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,    W_cut,   offsetd0p, offsetd1r):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        ##print ('i_src: {0}, i_dst: {1}'.format(i_src, i_dst))&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            ##print ('cuda.threadIdx.y: {0} cuda.blockDim.x: {1} cuda.threadIdx.x: {2}'.format(cuda.threadIdx.y, cuda.blockDim.x, cuda.threadIdx.x))&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            ##print('myidx: {0}, shifted: {1}'.format(myidx, offsidx))&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;            ##print ('After syncthreads: tmp_w[0]:{0} [1]:{1} [2]:{2}'.format(tmp_w[0], tmp_w[1], tmp_w[2]))&#10;&#10;            # Compute the location of d_src_ar, this defines what sigma will be. As r (as opp. to phi) increases, the sigma should increase.&#10;            M_f = float32(nfs)/(E2*math.log(((1+d_src_ar[i_src,1])/(2*E2))+1))&#10;&#10;            # Set some of M_f to 1 to ensure the fan-out starts at around the edge of the foveal region.&#10;            if (1+d_src_ar[i_src,1]) &lt; fovshift:&#10;                ##print ('reset M_f to M_f_start because 1+d_src_ar[i_src={2},1] (1+{0:f}) &lt; fovshift ({1:f})'.format(d_src_ar[i_src,1], fovshift, i_src))&#10;                M_f = M_f_start&#10;&#10;            # Compute modified sigma and 3 times this value&#10;            _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0 # as function of r, aka d_src_ar[1]. M_f is the function of r.&#10;            three_sigma = float32(3.0) * _sigma&#10;            ##print ('i_src: {4} i_dst: {5}. d_src_ar: ({0:f},{1:f}) d_dst_ar: ({2:f},{3:f})'.format(d_src_ar[i_src,0], d_src_ar[i_src,1], d_dst_ar[i_dst,0], d_dst_ar[i_dst,1], i_src, i_dst))&#10;            ##if M_f != M_f_start:&#10;                ##print ('sigma_m: {0:f}, M_f:{1:f}. M_f_start:{2:f}. sigma_0: {3:f} _sigma: {4:f} three_sigma: {5:f}'.format(sigma_m, M_f, M_f_start, sigma_0, _sigma, three_sigma))&#10;&#10;            # in-xy-plane distance (ignore d_src_ar[2]/dstdoc[2])&#10;            xd = (d_src_ar[i_src,0] - d_dst_ar[i_dst,0] + offsetd0p)&#10;            yd = (d_src_ar[i_src,1] - d_dst_ar[i_dst,1] + offsetd1r)&#10;            if abs(xd) &lt; three_sigma and abs(yd) &lt; three_sigma:&#10;                ##print('(abs(xd) abs({0:f}) &lt; three_sigma {1:f} and abs(yd) abs({2:f}) &lt; three_sigma)'.format(xd, yd, three_sigma))&#10;                dist = math.sqrt(math.pow(xd,2) + math.pow(yd,2))&#10;                gauss = math.exp(-0.5*math.pow(dist/_sigma,2))&#10;                #gauss = float32(d_src_ar[i_dst,1])&#10;                ##print ('i_src: {0} i_dst: {1} gauss: {2}'.format(i_src, i_dst, gauss))&#10;                if gauss &gt; W_cut:&#10;                    # Write result into weight_ar&#10;                    ##print('gauss {0:f} &gt; W_cut {1:f} - WRITE RESULTS.'.format(gauss, W_cut))&#10;                    tmp_w[offsidx] = float32(gauss)&#10;                    tmp_w[offsidx+1] = float32(i_src)&#10;                    tmp_w[offsidx+2] = float32(i_dst)&#10;                    ##print('tmp_w[{0}] = {1:f}, tmp_w[{2}] = {3:f}, tmp_w[{4}] = {5:f}'.format(offsidx, tmp_w[offsidx], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2]))&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;            ##print ('After set tmp_w (myidx={6}): tmp_w[{0}]:{1:f} [{2}]:{3:f} [{4}]:{5:f}'.format(offsidx, tmp_w[offsidx+0], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2], myidx))&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;                ##print ('tpb = {0}. tmp_w[0]:{1} [1]:{2} [2]:{3} [3]:{4}'.format(tpb, tmp_w[0], tmp_w[1], tmp_w[2], tmp_w[3]))&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    ##print('myidx: {0}, idx: {1}, shifted: {2}'.format(myidx, idx, offsidx2))&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    #if gt(theweight, W_cut): # Testing makes no difference to speed&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    ##if (theweight &gt; 0.0):&#10;                        ##print ('Set weight_ar[{0}]={1}'.format(weight_idx, theweight))&#10;                    weight_ar[weight_idx] = theweight&#10;                    ##if weight_idx &gt; 0:&#10;                        ##print ('weight_ar[{0}] = {1:f}'.format(weight_idx, theweight))&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    M_f_start=nfs/(E2*math.log((fovshift/(2*E2))+1))&#10;    print('M_f_start: {0:f}'.format(M_f_start))&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    print('Numpy arrays...')&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    print('cuda.to_device...')&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    #                                &quot;void(float32,   int32,  int32[:,:], int32[:,:], float32[:],  float32, float32,float32, float32,  float32, float32,int32,     int32)&#10;    time_start = int(round(time.time() * 1000))&#10;    dowork[blockspergrid, threadsperblock](M_f_start, nfs_sq, d_src_ar,   d_dst_ar,   d_weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,     W_cut,  _offsetd0p, _offsetd1r)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="offset_retgauss_gpu" sigma_m="15" E2="2.5" sigma_0="0.3" fovshift="1" nfs="150" W_cut="0.001" offsetd0p="-4" offsetd1r="0" max_n_weights="2e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_V1_p_edges_to_V2_pMp_rMr_syn0.bin" num_connections="650868" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="V1_p_edges to V2_pMp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile7.bin" num_elements="650868"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_p_edges to V2_pMp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="-2.07943" y="2.08044"/>
                    <curves>
                        <curve>
                            <C1 xpos="-1.0834" ypos="4.04935"/>
                            <C2 xpos="1.29501" ypos="3.33582"/>
                            <end xpos="1.36211" ypos="5.05419"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=sigma_m        #LOC=1,1&#10;#PARNAME=E2             #LOC=1,2&#10;#PARNAME=sigma_0        #LOC=2,1&#10;#PARNAME=fovshift       #LOC=2,2&#10;#PARNAME=nfs            #LOC=3,1&#10;#PARNAME=W_cut          #LOC=3,2&#10;#PARNAME=offsetd0p      #LOC=4,1&#10;#PARNAME=offsetd1r      #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# Compute a widening Gaussian connection function for a retinotopic&#10;# space, to maintain a constant Gaussian width in Cartesian&#10;# space. This is much like the WideningGaussian connection function,&#10;# but with a configurable neural field size width (W_nfs).&#10;#&#10;# This version incorporates an offset for dstloc[0] and dstloc[1] to&#10;# shift the Gaussian projection by a desired amount.&#10;#&#10;# Considering the r direction, r_d^max, the destination r value for&#10;# max connection strength for a given r_s is given by&#10;#&#10;# r_d^max = r_s + offsetd1r&#10;#&#10;# Thus for positive offsetd1r, &quot;connections are stronger in the&#10;# positive r direction away from the source&quot;.&#10;#&#10;# For positive offsetd0p, &quot;connections are stronger in the&#10;# positive p direction away from the source&quot;.&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsetd0p,offsetd1r,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Ensure these are fixed to being ints&#10;    _offsetd0p = int(offsetd0p)&#10;    _offsetd1r = int(offsetd1r)&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # To pad the arrays out to exact number of blocks&#10;        # arraysz1 = nfs_sq&#10;        #if arraysz%threadsperblock &gt; 0:&#10;        #    amod = arraysz%threadsperblock&#10;        #    print ('--\namod: {0}'.format(amod))&#10;        #    print ('arraysz: {0}'.format(arraysz))&#10;        #    print ('threadsperblock: {0}'.format(threadsperblock))&#10;        #    arrayszplus = arraysz + threadsperblock - amod&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;        #else:&#10;        #    print ('Set arrayszplus to arraysz1...')&#10;        #    arrayszplus = arraysz&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        #print ('j starts at {0}'.format(j))&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(float32,int32,  int32[:,:],int32[:,:],float32[:],float32, float32,float32, float32,  float32,float32, int32,     int32)&quot;)&#10;    def dowork (M_f_start,  nfs_sq, d_src_ar,  d_dst_ar,  weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,    W_cut,   offsetd0p, offsetd1r):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        ##print ('i_src: {0}, i_dst: {1}'.format(i_src, i_dst))&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            ##print ('cuda.threadIdx.y: {0} cuda.blockDim.x: {1} cuda.threadIdx.x: {2}'.format(cuda.threadIdx.y, cuda.blockDim.x, cuda.threadIdx.x))&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            ##print('myidx: {0}, shifted: {1}'.format(myidx, offsidx))&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;            ##print ('After syncthreads: tmp_w[0]:{0} [1]:{1} [2]:{2}'.format(tmp_w[0], tmp_w[1], tmp_w[2]))&#10;&#10;            # Compute the location of d_src_ar, this defines what sigma will be. As r (as opp. to phi) increases, the sigma should increase.&#10;            M_f = float32(nfs)/(E2*math.log(((1+d_src_ar[i_src,1])/(2*E2))+1))&#10;&#10;            # Set some of M_f to 1 to ensure the fan-out starts at around the edge of the foveal region.&#10;            if (1+d_src_ar[i_src,1]) &lt; fovshift:&#10;                ##print ('reset M_f to M_f_start because 1+d_src_ar[i_src={2},1] (1+{0:f}) &lt; fovshift ({1:f})'.format(d_src_ar[i_src,1], fovshift, i_src))&#10;                M_f = M_f_start&#10;&#10;            # Compute modified sigma and 3 times this value&#10;            _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0 # as function of r, aka d_src_ar[1]. M_f is the function of r.&#10;            three_sigma = float32(3.0) * _sigma&#10;            ##print ('i_src: {4} i_dst: {5}. d_src_ar: ({0:f},{1:f}) d_dst_ar: ({2:f},{3:f})'.format(d_src_ar[i_src,0], d_src_ar[i_src,1], d_dst_ar[i_dst,0], d_dst_ar[i_dst,1], i_src, i_dst))&#10;            ##if M_f != M_f_start:&#10;                ##print ('sigma_m: {0:f}, M_f:{1:f}. M_f_start:{2:f}. sigma_0: {3:f} _sigma: {4:f} three_sigma: {5:f}'.format(sigma_m, M_f, M_f_start, sigma_0, _sigma, three_sigma))&#10;&#10;            # in-xy-plane distance (ignore d_src_ar[2]/dstdoc[2])&#10;            xd = (d_src_ar[i_src,0] - d_dst_ar[i_dst,0] + offsetd0p)&#10;            yd = (d_src_ar[i_src,1] - d_dst_ar[i_dst,1] + offsetd1r)&#10;            if abs(xd) &lt; three_sigma and abs(yd) &lt; three_sigma:&#10;                ##print('(abs(xd) abs({0:f}) &lt; three_sigma {1:f} and abs(yd) abs({2:f}) &lt; three_sigma)'.format(xd, yd, three_sigma))&#10;                dist = math.sqrt(math.pow(xd,2) + math.pow(yd,2))&#10;                gauss = math.exp(-0.5*math.pow(dist/_sigma,2))&#10;                #gauss = float32(d_src_ar[i_dst,1])&#10;                ##print ('i_src: {0} i_dst: {1} gauss: {2}'.format(i_src, i_dst, gauss))&#10;                if gauss &gt; W_cut:&#10;                    # Write result into weight_ar&#10;                    ##print('gauss {0:f} &gt; W_cut {1:f} - WRITE RESULTS.'.format(gauss, W_cut))&#10;                    tmp_w[offsidx] = float32(gauss)&#10;                    tmp_w[offsidx+1] = float32(i_src)&#10;                    tmp_w[offsidx+2] = float32(i_dst)&#10;                    ##print('tmp_w[{0}] = {1:f}, tmp_w[{2}] = {3:f}, tmp_w[{4}] = {5:f}'.format(offsidx, tmp_w[offsidx], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2]))&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;            ##print ('After set tmp_w (myidx={6}): tmp_w[{0}]:{1:f} [{2}]:{3:f} [{4}]:{5:f}'.format(offsidx, tmp_w[offsidx+0], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2], myidx))&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;                ##print ('tpb = {0}. tmp_w[0]:{1} [1]:{2} [2]:{3} [3]:{4}'.format(tpb, tmp_w[0], tmp_w[1], tmp_w[2], tmp_w[3]))&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    ##print('myidx: {0}, idx: {1}, shifted: {2}'.format(myidx, idx, offsidx2))&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    #if gt(theweight, W_cut): # Testing makes no difference to speed&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    ##if (theweight &gt; 0.0):&#10;                        ##print ('Set weight_ar[{0}]={1}'.format(weight_idx, theweight))&#10;                    weight_ar[weight_idx] = theweight&#10;                    ##if weight_idx &gt; 0:&#10;                        ##print ('weight_ar[{0}] = {1:f}'.format(weight_idx, theweight))&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    M_f_start=nfs/(E2*math.log((fovshift/(2*E2))+1))&#10;    print('M_f_start: {0:f}'.format(M_f_start))&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    print('Numpy arrays...')&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    print('cuda.to_device...')&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    #                                &quot;void(float32,   int32,  int32[:,:], int32[:,:], float32[:],  float32, float32,float32, float32,  float32, float32,int32,     int32)&#10;    time_start = int(round(time.time() * 1000))&#10;    dowork[blockspergrid, threadsperblock](M_f_start, nfs_sq, d_src_ar,   d_dst_ar,   d_weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,     W_cut,  _offsetd0p, _offsetd1r)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="offset_retgauss_gpu" sigma_m="15" E2="2.5" sigma_0="0.3" fovshift="1" nfs="150" W_cut="0.001" offsetd0p="-4" offsetd1r="0" max_n_weights="2e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_V1_p_edges_to_V2_pMp_rPr_syn0.bin" num_connections="650868" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="V1_p_edges to V2_pMp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile10.bin" num_elements="650868"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_p_edges to V2_pMp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
    </LL:Population>
    <LL:Population>
        <LL:Annotation>
            <SpineCreator>
                <xPos value="0.933404"/>
                <yPos value="1.56237"/>
                <animSpeed value="0.2"/>
                <aspectRatio value="1.66667"/>
                <colour red="255" green="0" blue="0"/>
                <size value="1"/>
                <tag value="2"/>
                <x3D value="0"/>
                <y3D value="0"/>
                <z3D value="0"/>
                <is_visualised value="1"/>
            </SpineCreator>
        </LL:Annotation>
        <LL:Neuron name="V1_r_edges" size="22500" url="LINtanh.xml">
            <Property name="tau" dimension="?">
                <FixedValue value="10"/>
            </Property>
            <Property name="noise" dimension="?">
                <FixedValue value="0.01"/>
            </Property>
            <Property name="a" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </LL:Neuron>
        <Layout url="grid.xml" seed="123" minimum_distance="0">
            <Property name="numNeurons" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="rowlen" dimension="?">
                <FixedValue value="150"/>
            </Property>
            <Property name="x" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="z" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="t" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </Layout>
        <LL:Projection dst_population="V2_r_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="1.151" y="2.06237"/>
                    <curves>
                        <curve>
                            <C1 xpos="1.151" ypos="3.93652"/>
                            <C2 xpos="5.27956" ypos="3.97235"/>
                            <end xpos="5.26816" ypos="6.18373"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=sigma_m        #LOC=1,1&#10;#PARNAME=E2             #LOC=1,2&#10;#PARNAME=sigma_0        #LOC=2,1&#10;#PARNAME=fovshift       #LOC=2,2&#10;#PARNAME=nfs            #LOC=3,1&#10;#PARNAME=W_cut          #LOC=3,2&#10;#PARNAME=offsetd0p      #LOC=4,1&#10;#PARNAME=offsetd1r      #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# Compute a widening Gaussian connection function for a retinotopic&#10;# space, to maintain a constant Gaussian width in Cartesian&#10;# space. This is much like the WideningGaussian connection function,&#10;# but with a configurable neural field size width (W_nfs).&#10;#&#10;# This version incorporates an offset for dstloc[0] and dstloc[1] to&#10;# shift the Gaussian projection by a desired amount.&#10;#&#10;# ... AND it does so to dual Gaussians, offset by +/-offsetd0p and +/-offsetd1r&#10;#&#10;# offsetd0p is the +/- phi direction&#10;# offsetd1r is in the +/- r direction&#10;#&#10;# Considering the r direction, r_d^max, the destination r value for&#10;# max connection strength for a given r_s is given by&#10;#&#10;# r_d^max = r_s + offsetd1r&#10;#&#10;# Thus for positive offsetd1r, &quot;connections are stronger in the&#10;# positive r direction away from the source&quot;.&#10;#&#10;# For positive offsetd0p, &quot;connections are stronger in the&#10;# positive p direction away from the source&quot;.&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsetd0p,offsetd1r,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Ensure these are fixed to being ints&#10;    _offsetd0p = int(offsetd0p)&#10;    _offsetd1r = int(offsetd1r)&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # To pad the arrays out to exact number of blocks&#10;        # arraysz1 = nfs_sq&#10;        #if arraysz%threadsperblock &gt; 0:&#10;        #    amod = arraysz%threadsperblock&#10;        #    print ('--\namod: {0}'.format(amod))&#10;        #    print ('arraysz: {0}'.format(arraysz))&#10;        #    print ('threadsperblock: {0}'.format(threadsperblock))&#10;        #    arrayszplus = arraysz + threadsperblock - amod&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;        #else:&#10;        #    print ('Set arrayszplus to arraysz1...')&#10;        #    arrayszplus = arraysz&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        #print ('j starts at {0}'.format(j))&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(float32,int32,  int32[:,:],int32[:,:],float32[:],float32, float32,float32, float32,  float32,float32, int32,     int32)&quot;)&#10;    def dowork (M_f_start,  nfs_sq, d_src_ar,  d_dst_ar,  weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,    W_cut,   offsetd0p, offsetd1r):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        ##print ('i_src: {0}, i_dst: {1}'.format(i_src, i_dst))&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            ##print ('cuda.threadIdx.y: {0} cuda.blockDim.x: {1} cuda.threadIdx.x: {2}'.format(cuda.threadIdx.y, cuda.blockDim.x, cuda.threadIdx.x))&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            ##print('myidx: {0}, shifted: {1}'.format(myidx, offsidx))&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;            ##print ('After syncthreads: tmp_w[0]:{0} [1]:{1} [2]:{2}'.format(tmp_w[0], tmp_w[1], tmp_w[2]))&#10;&#10;            # Compute the location of d_src_ar, this defines what sigma will be. As r (as opp. to phi) increases, the sigma should increase.&#10;            M_f = float32(nfs)/(E2*math.log(((1+d_src_ar[i_src,1])/(2*E2))+1))&#10;&#10;            # Set some of M_f to 1 to ensure the fan-out starts at around the edge of the foveal region.&#10;            if (1+d_src_ar[i_src,1]) &lt; fovshift:&#10;                M_f = M_f_start&#10;&#10;            # Compute modified sigma and 3 times this value&#10;            _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0&#10;            #three_sigma = float32(3.0) * _sigma&#10;&#10;            # in-xy-plane distance (ignore d_src_ar[2]/dstdoc[2])&#10;            xd = (d_src_ar[i_src,0] - d_dst_ar[i_dst,0])# + offsetd0p)&#10;            yd = (d_src_ar[i_src,1] - d_dst_ar[i_dst,1])# + offsetd1r)&#10;&#10;            dist1 = math.sqrt(math.pow((xd+offsetd0p),2) + math.pow((yd+offsetd1r),2))&#10;            dist2 = math.sqrt(math.pow((xd-offsetd0p),2) + math.pow((yd-offsetd1r),2))&#10;            gauss1 = math.exp(-0.5*math.pow(dist1/_sigma,2))&#10;            gauss2 = math.exp(-0.5*math.pow(dist2/_sigma,2))&#10;&#10;            if gauss1 &gt; W_cut:&#10;                tmp_w[offsidx] = float32(gauss1)&#10;                tmp_w[offsidx+1] = float32(i_src)&#10;                tmp_w[offsidx+2] = float32(i_dst)&#10;            elif gauss2 &gt; W_cut:&#10;                tmp_w[offsidx] = float32(gauss2)&#10;                tmp_w[offsidx+1] = float32(i_src)&#10;                tmp_w[offsidx+2] = float32(i_dst)&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    weight_ar[weight_idx] = theweight&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    M_f_start=nfs/(E2*math.log((fovshift/(2*E2))+1))&#10;    print('M_f_start: {0:f}'.format(M_f_start))&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    print('Numpy arrays...')&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    print('cuda.to_device...')&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    #                                &quot;void(float32,   int32,  int32[:,:], int32[:,:], float32[:],  float32, float32,float32, float32,  float32, float32,int32,     int32)&#10;    time_start = int(round(time.time() * 1000))&#10;    dowork[blockspergrid, threadsperblock](M_f_start, nfs_sq, d_src_ar,   d_dst_ar,   d_weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,     W_cut,  _offsetd0p, _offsetd1r)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="offsetdual_retgauss_gpu" sigma_m="30" E2="2.5" sigma_0="0.3" fovshift="1" nfs="150" W_cut="0.001" offsetd0p="0" offsetd1r="4" max_n_weights="3e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_V1_r_edges_to_V2_r_lines_syn0.bin" num_connections="3890892" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="V1_r_edges to V2_r_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile8.bin" num_elements="3890892"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_r_edges to V2_r_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="0.548103" y="2.06237"/>
                    <curves>
                        <curve>
                            <C1 xpos="0.548103" ypos="4.00787"/>
                            <C2 xpos="-1.87656" ypos="2.7706"/>
                            <end xpos="-1.86779" ypos="5.14422"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=sigma_m        #LOC=1,1&#10;#PARNAME=E2             #LOC=1,2&#10;#PARNAME=sigma_0        #LOC=2,1&#10;#PARNAME=fovshift       #LOC=2,2&#10;#PARNAME=nfs            #LOC=3,1&#10;#PARNAME=W_cut          #LOC=3,2&#10;#PARNAME=offsetd0p      #LOC=4,1&#10;#PARNAME=offsetd1r      #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# Compute a widening Gaussian connection function for a retinotopic&#10;# space, to maintain a constant Gaussian width in Cartesian&#10;# space. This is much like the WideningGaussian connection function,&#10;# but with a configurable neural field size width (W_nfs).&#10;#&#10;# This version incorporates an offset for dstloc[0] and dstloc[1] to&#10;# shift the Gaussian projection by a desired amount.&#10;#&#10;# Considering the r direction, r_d^max, the destination r value for&#10;# max connection strength for a given r_s is given by&#10;#&#10;# r_d^max = r_s + offsetd1r&#10;#&#10;# Thus for positive offsetd1r, &quot;connections are stronger in the&#10;# positive r direction away from the source&quot;.&#10;#&#10;# For positive offsetd0p, &quot;connections are stronger in the&#10;# positive p direction away from the source&quot;.&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsetd0p,offsetd1r,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Ensure these are fixed to being ints&#10;    _offsetd0p = int(offsetd0p)&#10;    _offsetd1r = int(offsetd1r)&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # To pad the arrays out to exact number of blocks&#10;        # arraysz1 = nfs_sq&#10;        #if arraysz%threadsperblock &gt; 0:&#10;        #    amod = arraysz%threadsperblock&#10;        #    print ('--\namod: {0}'.format(amod))&#10;        #    print ('arraysz: {0}'.format(arraysz))&#10;        #    print ('threadsperblock: {0}'.format(threadsperblock))&#10;        #    arrayszplus = arraysz + threadsperblock - amod&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;        #else:&#10;        #    print ('Set arrayszplus to arraysz1...')&#10;        #    arrayszplus = arraysz&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        #print ('j starts at {0}'.format(j))&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(float32,int32,  int32[:,:],int32[:,:],float32[:],float32, float32,float32, float32,  float32,float32, int32,     int32)&quot;)&#10;    def dowork (M_f_start,  nfs_sq, d_src_ar,  d_dst_ar,  weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,    W_cut,   offsetd0p, offsetd1r):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        ##print ('i_src: {0}, i_dst: {1}'.format(i_src, i_dst))&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            ##print ('cuda.threadIdx.y: {0} cuda.blockDim.x: {1} cuda.threadIdx.x: {2}'.format(cuda.threadIdx.y, cuda.blockDim.x, cuda.threadIdx.x))&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            ##print('myidx: {0}, shifted: {1}'.format(myidx, offsidx))&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;            ##print ('After syncthreads: tmp_w[0]:{0} [1]:{1} [2]:{2}'.format(tmp_w[0], tmp_w[1], tmp_w[2]))&#10;&#10;            # Compute the location of d_src_ar, this defines what sigma will be. As r (as opp. to phi) increases, the sigma should increase.&#10;            M_f = float32(nfs)/(E2*math.log(((1+d_src_ar[i_src,1])/(2*E2))+1))&#10;&#10;            # Set some of M_f to 1 to ensure the fan-out starts at around the edge of the foveal region.&#10;            if (1+d_src_ar[i_src,1]) &lt; fovshift:&#10;                ##print ('reset M_f to M_f_start because 1+d_src_ar[i_src={2},1] (1+{0:f}) &lt; fovshift ({1:f})'.format(d_src_ar[i_src,1], fovshift, i_src))&#10;                M_f = M_f_start&#10;&#10;            # Compute modified sigma and 3 times this value&#10;            _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0 # as function of r, aka d_src_ar[1]. M_f is the function of r.&#10;            three_sigma = float32(3.0) * _sigma&#10;            ##print ('i_src: {4} i_dst: {5}. d_src_ar: ({0:f},{1:f}) d_dst_ar: ({2:f},{3:f})'.format(d_src_ar[i_src,0], d_src_ar[i_src,1], d_dst_ar[i_dst,0], d_dst_ar[i_dst,1], i_src, i_dst))&#10;            ##if M_f != M_f_start:&#10;                ##print ('sigma_m: {0:f}, M_f:{1:f}. M_f_start:{2:f}. sigma_0: {3:f} _sigma: {4:f} three_sigma: {5:f}'.format(sigma_m, M_f, M_f_start, sigma_0, _sigma, three_sigma))&#10;&#10;            # in-xy-plane distance (ignore d_src_ar[2]/dstdoc[2])&#10;            xd = (d_src_ar[i_src,0] - d_dst_ar[i_dst,0] + offsetd0p)&#10;            yd = (d_src_ar[i_src,1] - d_dst_ar[i_dst,1] + offsetd1r)&#10;            if abs(xd) &lt; three_sigma and abs(yd) &lt; three_sigma:&#10;                ##print('(abs(xd) abs({0:f}) &lt; three_sigma {1:f} and abs(yd) abs({2:f}) &lt; three_sigma)'.format(xd, yd, three_sigma))&#10;                dist = math.sqrt(math.pow(xd,2) + math.pow(yd,2))&#10;                gauss = math.exp(-0.5*math.pow(dist/_sigma,2))&#10;                #gauss = float32(d_src_ar[i_dst,1])&#10;                ##print ('i_src: {0} i_dst: {1} gauss: {2}'.format(i_src, i_dst, gauss))&#10;                if gauss &gt; W_cut:&#10;                    # Write result into weight_ar&#10;                    ##print('gauss {0:f} &gt; W_cut {1:f} - WRITE RESULTS.'.format(gauss, W_cut))&#10;                    tmp_w[offsidx] = float32(gauss)&#10;                    tmp_w[offsidx+1] = float32(i_src)&#10;                    tmp_w[offsidx+2] = float32(i_dst)&#10;                    ##print('tmp_w[{0}] = {1:f}, tmp_w[{2}] = {3:f}, tmp_w[{4}] = {5:f}'.format(offsidx, tmp_w[offsidx], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2]))&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;            ##print ('After set tmp_w (myidx={6}): tmp_w[{0}]:{1:f} [{2}]:{3:f} [{4}]:{5:f}'.format(offsidx, tmp_w[offsidx+0], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2], myidx))&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;                ##print ('tpb = {0}. tmp_w[0]:{1} [1]:{2} [2]:{3} [3]:{4}'.format(tpb, tmp_w[0], tmp_w[1], tmp_w[2], tmp_w[3]))&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    ##print('myidx: {0}, idx: {1}, shifted: {2}'.format(myidx, idx, offsidx2))&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    #if gt(theweight, W_cut): # Testing makes no difference to speed&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    ##if (theweight &gt; 0.0):&#10;                        ##print ('Set weight_ar[{0}]={1}'.format(weight_idx, theweight))&#10;                    weight_ar[weight_idx] = theweight&#10;                    ##if weight_idx &gt; 0:&#10;                        ##print ('weight_ar[{0}] = {1:f}'.format(weight_idx, theweight))&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    M_f_start=nfs/(E2*math.log((fovshift/(2*E2))+1))&#10;    print('M_f_start: {0:f}'.format(M_f_start))&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    print('Numpy arrays...')&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    print('cuda.to_device...')&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    #                                &quot;void(float32,   int32,  int32[:,:], int32[:,:], float32[:],  float32, float32,float32, float32,  float32, float32,int32,     int32)&#10;    time_start = int(round(time.time() * 1000))&#10;    dowork[blockspergrid, threadsperblock](M_f_start, nfs_sq, d_src_ar,   d_dst_ar,   d_weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,     W_cut,  _offsetd0p, _offsetd1r)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="offset_retgauss_gpu" sigma_m="15" E2="2.5" sigma_0="0.3" fovshift="1" nfs="150" W_cut="0.001" offsetd0p="0" offsetd1r="4" max_n_weights="2e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_V1_r_edges_to_V2_pPp_rPr_syn0.bin" num_connections="641100" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="V1_r_edges to V2_pPp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile9.bin" num_elements="641100"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_r_edges to V2_pPp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V1_p_edges">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="0.100069" y="1.46067"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.598272" ypos="1.31002"/>
                            <C2 xpos="-0.788304" ypos="1.30539"/>
                            <end xpos="-1.52656" ypos="1.53987"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V1_r_edges to V1_p_edges Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_r_edges to V1_p_edges Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="0.538587" y="2.06237"/>
                    <curves>
                        <curve>
                            <C1 xpos="0.170843" ypos="6.96097"/>
                            <C2 xpos="-1.49104" ypos="6.02476"/>
                            <end xpos="-1.7654" ypos="7.25046"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=sigma_m        #LOC=1,1&#10;#PARNAME=E2             #LOC=1,2&#10;#PARNAME=sigma_0        #LOC=2,1&#10;#PARNAME=fovshift       #LOC=2,2&#10;#PARNAME=nfs            #LOC=3,1&#10;#PARNAME=W_cut          #LOC=3,2&#10;#PARNAME=offsetd0p      #LOC=4,1&#10;#PARNAME=offsetd1r      #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# Compute a widening Gaussian connection function for a retinotopic&#10;# space, to maintain a constant Gaussian width in Cartesian&#10;# space. This is much like the WideningGaussian connection function,&#10;# but with a configurable neural field size width (W_nfs).&#10;#&#10;# This version incorporates an offset for dstloc[0] and dstloc[1] to&#10;# shift the Gaussian projection by a desired amount.&#10;#&#10;# Considering the r direction, r_d^max, the destination r value for&#10;# max connection strength for a given r_s is given by&#10;#&#10;# r_d^max = r_s + offsetd1r&#10;#&#10;# Thus for positive offsetd1r, &quot;connections are stronger in the&#10;# positive r direction away from the source&quot;.&#10;#&#10;# For positive offsetd0p, &quot;connections are stronger in the&#10;# positive p direction away from the source&quot;.&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsetd0p,offsetd1r,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Ensure these are fixed to being ints&#10;    _offsetd0p = int(offsetd0p)&#10;    _offsetd1r = int(offsetd1r)&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # To pad the arrays out to exact number of blocks&#10;        # arraysz1 = nfs_sq&#10;        #if arraysz%threadsperblock &gt; 0:&#10;        #    amod = arraysz%threadsperblock&#10;        #    print ('--\namod: {0}'.format(amod))&#10;        #    print ('arraysz: {0}'.format(arraysz))&#10;        #    print ('threadsperblock: {0}'.format(threadsperblock))&#10;        #    arrayszplus = arraysz + threadsperblock - amod&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;        #else:&#10;        #    print ('Set arrayszplus to arraysz1...')&#10;        #    arrayszplus = arraysz&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        #print ('j starts at {0}'.format(j))&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(float32,int32,  int32[:,:],int32[:,:],float32[:],float32, float32,float32, float32,  float32,float32, int32,     int32)&quot;)&#10;    def dowork (M_f_start,  nfs_sq, d_src_ar,  d_dst_ar,  weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,    W_cut,   offsetd0p, offsetd1r):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        ##print ('i_src: {0}, i_dst: {1}'.format(i_src, i_dst))&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            ##print ('cuda.threadIdx.y: {0} cuda.blockDim.x: {1} cuda.threadIdx.x: {2}'.format(cuda.threadIdx.y, cuda.blockDim.x, cuda.threadIdx.x))&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            ##print('myidx: {0}, shifted: {1}'.format(myidx, offsidx))&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;            ##print ('After syncthreads: tmp_w[0]:{0} [1]:{1} [2]:{2}'.format(tmp_w[0], tmp_w[1], tmp_w[2]))&#10;&#10;            # Compute the location of d_src_ar, this defines what sigma will be. As r (as opp. to phi) increases, the sigma should increase.&#10;            M_f = float32(nfs)/(E2*math.log(((1+d_src_ar[i_src,1])/(2*E2))+1))&#10;&#10;            # Set some of M_f to 1 to ensure the fan-out starts at around the edge of the foveal region.&#10;            if (1+d_src_ar[i_src,1]) &lt; fovshift:&#10;                ##print ('reset M_f to M_f_start because 1+d_src_ar[i_src={2},1] (1+{0:f}) &lt; fovshift ({1:f})'.format(d_src_ar[i_src,1], fovshift, i_src))&#10;                M_f = M_f_start&#10;&#10;            # Compute modified sigma and 3 times this value&#10;            _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0 # as function of r, aka d_src_ar[1]. M_f is the function of r.&#10;            three_sigma = float32(3.0) * _sigma&#10;            ##print ('i_src: {4} i_dst: {5}. d_src_ar: ({0:f},{1:f}) d_dst_ar: ({2:f},{3:f})'.format(d_src_ar[i_src,0], d_src_ar[i_src,1], d_dst_ar[i_dst,0], d_dst_ar[i_dst,1], i_src, i_dst))&#10;            ##if M_f != M_f_start:&#10;                ##print ('sigma_m: {0:f}, M_f:{1:f}. M_f_start:{2:f}. sigma_0: {3:f} _sigma: {4:f} three_sigma: {5:f}'.format(sigma_m, M_f, M_f_start, sigma_0, _sigma, three_sigma))&#10;&#10;            # in-xy-plane distance (ignore d_src_ar[2]/dstdoc[2])&#10;            xd = (d_src_ar[i_src,0] - d_dst_ar[i_dst,0] + offsetd0p)&#10;            yd = (d_src_ar[i_src,1] - d_dst_ar[i_dst,1] + offsetd1r)&#10;            if abs(xd) &lt; three_sigma and abs(yd) &lt; three_sigma:&#10;                ##print('(abs(xd) abs({0:f}) &lt; three_sigma {1:f} and abs(yd) abs({2:f}) &lt; three_sigma)'.format(xd, yd, three_sigma))&#10;                dist = math.sqrt(math.pow(xd,2) + math.pow(yd,2))&#10;                gauss = math.exp(-0.5*math.pow(dist/_sigma,2))&#10;                #gauss = float32(d_src_ar[i_dst,1])&#10;                ##print ('i_src: {0} i_dst: {1} gauss: {2}'.format(i_src, i_dst, gauss))&#10;                if gauss &gt; W_cut:&#10;                    # Write result into weight_ar&#10;                    ##print('gauss {0:f} &gt; W_cut {1:f} - WRITE RESULTS.'.format(gauss, W_cut))&#10;                    tmp_w[offsidx] = float32(gauss)&#10;                    tmp_w[offsidx+1] = float32(i_src)&#10;                    tmp_w[offsidx+2] = float32(i_dst)&#10;                    ##print('tmp_w[{0}] = {1:f}, tmp_w[{2}] = {3:f}, tmp_w[{4}] = {5:f}'.format(offsidx, tmp_w[offsidx], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2]))&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;            ##print ('After set tmp_w (myidx={6}): tmp_w[{0}]:{1:f} [{2}]:{3:f} [{4}]:{5:f}'.format(offsidx, tmp_w[offsidx+0], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2], myidx))&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;                ##print ('tpb = {0}. tmp_w[0]:{1} [1]:{2} [2]:{3} [3]:{4}'.format(tpb, tmp_w[0], tmp_w[1], tmp_w[2], tmp_w[3]))&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    ##print('myidx: {0}, idx: {1}, shifted: {2}'.format(myidx, idx, offsidx2))&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    #if gt(theweight, W_cut): # Testing makes no difference to speed&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    ##if (theweight &gt; 0.0):&#10;                        ##print ('Set weight_ar[{0}]={1}'.format(weight_idx, theweight))&#10;                    weight_ar[weight_idx] = theweight&#10;                    ##if weight_idx &gt; 0:&#10;                        ##print ('weight_ar[{0}] = {1:f}'.format(weight_idx, theweight))&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    M_f_start=nfs/(E2*math.log((fovshift/(2*E2))+1))&#10;    print('M_f_start: {0:f}'.format(M_f_start))&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    print('Numpy arrays...')&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    print('cuda.to_device...')&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    #                                &quot;void(float32,   int32,  int32[:,:], int32[:,:], float32[:],  float32, float32,float32, float32,  float32, float32,int32,     int32)&#10;    time_start = int(round(time.time() * 1000))&#10;    dowork[blockspergrid, threadsperblock](M_f_start, nfs_sq, d_src_ar,   d_dst_ar,   d_weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,     W_cut,  _offsetd0p, _offsetd1r)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="offset_retgauss_gpu" sigma_m="15" E2="2.5" sigma_0="0.3" fovshift="1" nfs="150" W_cut="0.001" offsetd0p="0" offsetd1r="-4" max_n_weights="2e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_V1_r_edges_to_V2_pPp_rMr_syn0.bin" num_connections="664822" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="V1_r_edges to V2_pPp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile4.bin" num_elements="664822"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_r_edges to V2_pPp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="0.535335" y="2.06237"/>
                    <curves>
                        <curve>
                            <C1 xpos="0.574324" ypos="5.72301"/>
                            <C2 xpos="-0.0279321" ypos="4.85782"/>
                            <end xpos="1.79036" ypos="7.14492"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=sigma_m        #LOC=1,1&#10;#PARNAME=E2             #LOC=1,2&#10;#PARNAME=sigma_0        #LOC=2,1&#10;#PARNAME=fovshift       #LOC=2,2&#10;#PARNAME=nfs            #LOC=3,1&#10;#PARNAME=W_cut          #LOC=3,2&#10;#PARNAME=offsetd0p      #LOC=4,1&#10;#PARNAME=offsetd1r      #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# Compute a widening Gaussian connection function for a retinotopic&#10;# space, to maintain a constant Gaussian width in Cartesian&#10;# space. This is much like the WideningGaussian connection function,&#10;# but with a configurable neural field size width (W_nfs).&#10;#&#10;# This version incorporates an offset for dstloc[0] and dstloc[1] to&#10;# shift the Gaussian projection by a desired amount.&#10;#&#10;# Considering the r direction, r_d^max, the destination r value for&#10;# max connection strength for a given r_s is given by&#10;#&#10;# r_d^max = r_s + offsetd1r&#10;#&#10;# Thus for positive offsetd1r, &quot;connections are stronger in the&#10;# positive r direction away from the source&quot;.&#10;#&#10;# For positive offsetd0p, &quot;connections are stronger in the&#10;# positive p direction away from the source&quot;.&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsetd0p,offsetd1r,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Ensure these are fixed to being ints&#10;    _offsetd0p = int(offsetd0p)&#10;    _offsetd1r = int(offsetd1r)&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # To pad the arrays out to exact number of blocks&#10;        # arraysz1 = nfs_sq&#10;        #if arraysz%threadsperblock &gt; 0:&#10;        #    amod = arraysz%threadsperblock&#10;        #    print ('--\namod: {0}'.format(amod))&#10;        #    print ('arraysz: {0}'.format(arraysz))&#10;        #    print ('threadsperblock: {0}'.format(threadsperblock))&#10;        #    arrayszplus = arraysz + threadsperblock - amod&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;        #else:&#10;        #    print ('Set arrayszplus to arraysz1...')&#10;        #    arrayszplus = arraysz&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        #print ('j starts at {0}'.format(j))&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(float32,int32,  int32[:,:],int32[:,:],float32[:],float32, float32,float32, float32,  float32,float32, int32,     int32)&quot;)&#10;    def dowork (M_f_start,  nfs_sq, d_src_ar,  d_dst_ar,  weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,    W_cut,   offsetd0p, offsetd1r):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        ##print ('i_src: {0}, i_dst: {1}'.format(i_src, i_dst))&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            ##print ('cuda.threadIdx.y: {0} cuda.blockDim.x: {1} cuda.threadIdx.x: {2}'.format(cuda.threadIdx.y, cuda.blockDim.x, cuda.threadIdx.x))&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            ##print('myidx: {0}, shifted: {1}'.format(myidx, offsidx))&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;            ##print ('After syncthreads: tmp_w[0]:{0} [1]:{1} [2]:{2}'.format(tmp_w[0], tmp_w[1], tmp_w[2]))&#10;&#10;            # Compute the location of d_src_ar, this defines what sigma will be. As r (as opp. to phi) increases, the sigma should increase.&#10;            M_f = float32(nfs)/(E2*math.log(((1+d_src_ar[i_src,1])/(2*E2))+1))&#10;&#10;            # Set some of M_f to 1 to ensure the fan-out starts at around the edge of the foveal region.&#10;            if (1+d_src_ar[i_src,1]) &lt; fovshift:&#10;                ##print ('reset M_f to M_f_start because 1+d_src_ar[i_src={2},1] (1+{0:f}) &lt; fovshift ({1:f})'.format(d_src_ar[i_src,1], fovshift, i_src))&#10;                M_f = M_f_start&#10;&#10;            # Compute modified sigma and 3 times this value&#10;            _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0 # as function of r, aka d_src_ar[1]. M_f is the function of r.&#10;            three_sigma = float32(3.0) * _sigma&#10;            ##print ('i_src: {4} i_dst: {5}. d_src_ar: ({0:f},{1:f}) d_dst_ar: ({2:f},{3:f})'.format(d_src_ar[i_src,0], d_src_ar[i_src,1], d_dst_ar[i_dst,0], d_dst_ar[i_dst,1], i_src, i_dst))&#10;            ##if M_f != M_f_start:&#10;                ##print ('sigma_m: {0:f}, M_f:{1:f}. M_f_start:{2:f}. sigma_0: {3:f} _sigma: {4:f} three_sigma: {5:f}'.format(sigma_m, M_f, M_f_start, sigma_0, _sigma, three_sigma))&#10;&#10;            # in-xy-plane distance (ignore d_src_ar[2]/dstdoc[2])&#10;            xd = (d_src_ar[i_src,0] - d_dst_ar[i_dst,0] + offsetd0p)&#10;            yd = (d_src_ar[i_src,1] - d_dst_ar[i_dst,1] + offsetd1r)&#10;            if abs(xd) &lt; three_sigma and abs(yd) &lt; three_sigma:&#10;                ##print('(abs(xd) abs({0:f}) &lt; three_sigma {1:f} and abs(yd) abs({2:f}) &lt; three_sigma)'.format(xd, yd, three_sigma))&#10;                dist = math.sqrt(math.pow(xd,2) + math.pow(yd,2))&#10;                gauss = math.exp(-0.5*math.pow(dist/_sigma,2))&#10;                #gauss = float32(d_src_ar[i_dst,1])&#10;                ##print ('i_src: {0} i_dst: {1} gauss: {2}'.format(i_src, i_dst, gauss))&#10;                if gauss &gt; W_cut:&#10;                    # Write result into weight_ar&#10;                    ##print('gauss {0:f} &gt; W_cut {1:f} - WRITE RESULTS.'.format(gauss, W_cut))&#10;                    tmp_w[offsidx] = float32(gauss)&#10;                    tmp_w[offsidx+1] = float32(i_src)&#10;                    tmp_w[offsidx+2] = float32(i_dst)&#10;                    ##print('tmp_w[{0}] = {1:f}, tmp_w[{2}] = {3:f}, tmp_w[{4}] = {5:f}'.format(offsidx, tmp_w[offsidx], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2]))&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;            ##print ('After set tmp_w (myidx={6}): tmp_w[{0}]:{1:f} [{2}]:{3:f} [{4}]:{5:f}'.format(offsidx, tmp_w[offsidx+0], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2], myidx))&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;                ##print ('tpb = {0}. tmp_w[0]:{1} [1]:{2} [2]:{3} [3]:{4}'.format(tpb, tmp_w[0], tmp_w[1], tmp_w[2], tmp_w[3]))&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    ##print('myidx: {0}, idx: {1}, shifted: {2}'.format(myidx, idx, offsidx2))&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    #if gt(theweight, W_cut): # Testing makes no difference to speed&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    ##if (theweight &gt; 0.0):&#10;                        ##print ('Set weight_ar[{0}]={1}'.format(weight_idx, theweight))&#10;                    weight_ar[weight_idx] = theweight&#10;                    ##if weight_idx &gt; 0:&#10;                        ##print ('weight_ar[{0}] = {1:f}'.format(weight_idx, theweight))&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    M_f_start=nfs/(E2*math.log((fovshift/(2*E2))+1))&#10;    print('M_f_start: {0:f}'.format(M_f_start))&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    print('Numpy arrays...')&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    print('cuda.to_device...')&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    #                                &quot;void(float32,   int32,  int32[:,:], int32[:,:], float32[:],  float32, float32,float32, float32,  float32, float32,int32,     int32)&#10;    time_start = int(round(time.time() * 1000))&#10;    dowork[blockspergrid, threadsperblock](M_f_start, nfs_sq, d_src_ar,   d_dst_ar,   d_weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,     W_cut,  _offsetd0p, _offsetd1r)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="offset_retgauss_gpu" sigma_m="15" E2="2.5" sigma_0="0.3" fovshift="1" nfs="150" W_cut="0.001" offsetd0p="0" offsetd1r="-4" max_n_weights="2e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_V1_r_edges_to_V2_pMp_rMr_syn0.bin" num_connections="664822" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="V1_r_edges to V2_pMp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile12.bin" num_elements="664822"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_r_edges to V2_pMp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="4" showlabel="0"/>
                    <start x="0.519121" y="2.06237"/>
                    <curves>
                        <curve>
                            <C1 xpos="0.688519" ypos="4.31097"/>
                            <C2 xpos="1.76973" ypos="3.66235"/>
                            <end xpos="1.76973" ypos="5.05419"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <ConnectionList>
                    <LL:Annotation>
                        <SpineCreator>
                            <Script text="#PARNAME=sigma_m        #LOC=1,1&#10;#PARNAME=E2             #LOC=1,2&#10;#PARNAME=sigma_0        #LOC=2,1&#10;#PARNAME=fovshift       #LOC=2,2&#10;#PARNAME=nfs            #LOC=3,1&#10;#PARNAME=W_cut          #LOC=3,2&#10;#PARNAME=offsetd0p      #LOC=4,1&#10;#PARNAME=offsetd1r      #LOC=4,2&#10;#PARNAME=max_n_weights  #LOC=5,1&#10;#HASWEIGHT&#10;&#10;# Compute a widening Gaussian connection function for a retinotopic&#10;# space, to maintain a constant Gaussian width in Cartesian&#10;# space. This is much like the WideningGaussian connection function,&#10;# but with a configurable neural field size width (W_nfs).&#10;#&#10;# This version incorporates an offset for dstloc[0] and dstloc[1] to&#10;# shift the Gaussian projection by a desired amount.&#10;#&#10;# Considering the r direction, r_d^max, the destination r value for&#10;# max connection strength for a given r_s is given by&#10;#&#10;# r_d^max = r_s + offsetd1r&#10;#&#10;# Thus for positive offsetd1r, &quot;connections are stronger in the&#10;# positive r direction away from the source&quot;.&#10;#&#10;# For positive offsetd0p, &quot;connections are stronger in the&#10;# positive p direction away from the source&quot;.&#10;#&#10;# max_n_weights is the size of the array to allocate to contain the&#10;# generated weights. In principle this could be nfs^4, but for a 150&#10;# side neural field side, that would result in a requirement for 8 GB&#10;# of device RAM. Instead, specify max_n_weights and hope you chose&#10;# enough! 20,000,000 is reasonable.&#10;&#10;def connectionFunc(srclocs,dstlocs,sigma_m,E2,sigma_0,fovshift,nfs,W_cut,offsetd0p,offsetd1r,max_n_weights):&#10;&#10;    from numba import cuda, float32, int32&#10;    import math&#10;    import numpy as np&#10;    from operator import gt&#10;    import time # for code profiling&#10;&#10;    # Ensure these are fixed to being ints&#10;    _offsetd0p = int(offsetd0p)&#10;    _offsetd1r = int(offsetd1r)&#10;&#10;    # Shifts index to avoid bank conflicts (on GTX1070)&#10;    @cuda.jit(device=True)&#10;    def shifted_idx (idx):&#10;        num_banks = 16 # 16 and 768 works for GTX1070/Compute capability 6.1; makes 48 KB of shared memory. GTX 1080 May have 96 KB; thus double but it's still CC 6.1.&#10;        bank_width_int32 = 768&#10;        # max_idx = (bank_width_int32 * num_banks)&#10;        idx_idiv_num_banks = idx // num_banks&#10;        idx_mod_num_banks = idx % num_banks&#10;        offs_idx = ((bank_width_int32 * idx_mod_num_banks) + (idx_idiv_num_banks))&#10;        return offs_idx&#10;&#10;    @cuda.jit(device=True)&#10;    def shifted_idx3 (thread_idx):&#10;        # How many int32 memory locations being used in each thread:&#10;        memorywidth = 3 # 12//4&#10;        # The width of a bank in int32s:&#10;        bank_width_int32 = 192 # 768//4&#10;        #num_banks = 16&#10;        bankwidth = 3072 # bank_width_int32 * num_banks&#10;&#10;        offs_idx = (bank_width_int32 * thread_idx)&#10;        idx_idiv_bw = offs_idx // bankwidth&#10;        idx_mod_bw = offs_idx % bankwidth&#10;        offs_idx = idx_mod_bw + idx_idiv_bw * memorywidth&#10;&#10;        return offs_idx&#10;&#10;    ### GPU SCAN CODE GOES HERE&#10;    ###############################################################################&#10;    # Like prefixscan_gpu, but returning the non-zero values from&#10;    # weight_ar in d_out&#10;    #&#10;    # d_weight_ar - A memory area on the GPU memory containing a sparse&#10;    # matrix of results (floating point, probably)&#10;    #&#10;    # arraysz - The length of the sparse matrix contained in&#10;    # d_weight_ar (this will be the same as nfs_sq * nfs_sq)&#10;    #&#10;    # arrayszplus - The size of the memory area d_weight_ar. This should&#10;    # be an integer multiple of threadsperblock&#10;    #&#10;    # threadsperblock - how many threads to launch per threadblock on the&#10;    # GPU.&#10;    #&#10;    # d_out - Array for results in connectionFunc output format: 4&#10;    # columns of data. Populated by extract_nonzero()&#10;    #&#10;    # _nfs_sq square of neural field size (nfs is the side length of&#10;    # the square neural field).&#10;    #&#10;    def reduce_nonzero_gpu (d_weight_ar, arraysz, arrayszplus, threadsperblock, _d_out, _nfs_sq):&#10;        import math&#10;        from operator import gt&#10;&#10;        # Detect non-zero values in weight_ar_. Update each element of the&#10;        # identically shaped nonzero_ar_ to hold 1 for non-zero values in&#10;        # weight_ar_ and 0 for zero values in weight_ar_&#10;        @cuda.jit(&quot;void(float32[:], int32[:], int32)&quot;)&#10;        def detect_nonzero (weight_ar_, nonzero_ar_, arraysz):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            if thid &lt; arraysz:&#10;                nonzero_ar_[thid] = 1 if weight_ar_[thid] &gt; 0.0 else 0&#10;                # debug:&#10;                #if nonzero_ar_[thid] == 1:&#10;                #    print ('nonzero_ar_[{0}] = {1}, weight_ar_[{0}] = {2}'.format(thid, nonzero_ar_[thid], weight_ar_[thid]))&#10;&#10;        #&#10;        # parallel prefix scan for stream compaction (See sect. 39.3&#10;        # https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html)&#10;        #&#10;        # This sums up the values in input_ar_, placing the running&#10;        # total in scan_ar_. The final carry value is placed in carry_&#10;        #&#10;        # This kernel carries out the prefix scan for a single threadblock.&#10;        #&#10;        # scan_ar_ - The result of the prefix scan. Array of uint32s&#10;        # (could be float32s)&#10;        #&#10;        # input_ar_ - The input for the algorithm. Array of uint32s (could&#10;        # be float32s)&#10;        #&#10;        # carry_ - The carry array - carry_[cuda.blockIdx.x] is updated&#10;        # with the final value in scan_ar_&#10;        #&#10;        # threadsperblock_ - The number of CUDA threads per threadblock&#10;        #&#10;        # inputsz - The size of the arrays scan_ar_ and input_ar_&#10;        #&#10;        @cuda.jit()&#10;        def one_block_scan (scan_ar_, input_ar_, carry_, threadsperblock_, inputsz):&#10;            thid = cuda.threadIdx.x                     # Thread ID&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # Threadblock offset&#10;            d = threadsperblock_//2                     # d starts out as half the block&#10;&#10;            # This runs for every element in input_ar_&#10;            if (thid+tb_offset) &lt; (inputsz-d):&#10;&#10;                # Allocate ALL shared memory here. Use float32 as type; could be uint32.&#10;                shmem = cuda.shared.array(12288, dtype=float32)&#10;                ai = thid # within one block&#10;                bi = ai + d # ai and bi are well separated across the shared memory. bi = ai+1 could also work?&#10;&#10;                # Compute shifted indices for efficient use of shared memory&#10;                ai_s = shifted_idx(ai)&#10;                bi_s = shifted_idx(bi)&#10;&#10;                # Copy input into local shared memory array&#10;                shmem[ai_s] = input_ar_[ai+tb_offset]&#10;                shmem[bi_s] = input_ar_[bi+tb_offset]&#10;&#10;                offset = 1&#10;                # Upsweep: Build sum in place up the tree&#10;                while (d &gt; 0):&#10;                    cuda.syncthreads()&#10;                    if thid &lt; d:&#10;                        # Block B&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        shmem[bi_s] += shmem[ai_s]&#10;&#10;                    offset *= 2&#10;                    d &gt;&gt;= 1&#10;&#10;                cuda.syncthreads()&#10;&#10;                # Block C: clear the last element - the first step of the downsweep&#10;                if (thid == 0):&#10;                    nm1s = shifted_idx(threadsperblock-1)&#10;                    # Carry last number in the block&#10;                    carry_[cuda.blockIdx.x] = shmem[nm1s];&#10;                    shmem[nm1s] = 0&#10;&#10;                # Downsweep: traverse down tree &amp; build scan&#10;                d = 1&#10;                while d &lt; threadsperblock_:&#10;                    offset &gt;&gt;= 1&#10;                    cuda.syncthreads()&#10;                    if (thid &lt; d):&#10;                        # Block D&#10;                        ai = offset*(2*thid+1)-1&#10;                        bi = offset*(2*thid+2)-1&#10;                        ai_s = shifted_idx(ai)&#10;                        bi_s = shifted_idx(bi)&#10;                        t = shmem[ai_s]&#10;                        shmem[ai_s] = shmem[bi_s]&#10;                        shmem[bi_s] += t&#10;                    d *= 2&#10;                cuda.syncthreads()&#10;&#10;                # Block E: write results to device memory&#10;                scan_ar_[ai+tb_offset] = shmem[ai_s]&#10;                if bi &lt; threadsperblock_:&#10;                    scan_ar_[bi+tb_offset] = shmem[bi_s]&#10;                    #print ('Set scan_ar_[{0}] to shmem[{1}] = {2}'.format(bi+tb_offset, bi_s, shmem[bi_s]))&#10;&#10;            return # End of one_block_scan()&#10;&#10;        # Last job is to add on the carry to each part of scan_ar WHILE AT THE SAME TIME SUMMING WITHIN A BLOCK&#10;        @cuda.jit&#10;        def sum_scans(new_carry_ar_, scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            # Try replacing with ternarys at some point:&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;            elif cuda.blockIdx.x == 0 and arr_addr &lt; scan_ar_sz:&#10;                new_carry_ar_[arr_addr] = scan_ar_[arr_addr]&#10;&#10;            cuda.syncthreads()&#10;&#10;        @cuda.jit&#10;        def sum_scans_destructively(scan_ar_, scan_ar_sz, carry_ar_):&#10;            thid = cuda.threadIdx.x&#10;            tb_offset = cuda.blockIdx.x*cuda.blockDim.x # threadblock offset&#10;            arr_addr = thid+tb_offset&#10;&#10;            if cuda.blockIdx.x &gt; 0 and arr_addr &lt; scan_ar_sz:&#10;                #print ('scan_ar_[{0}] += carry_ar_[{1}]  ||| {2} += {3}'&#10;                #       .format(arr_addr, cuda.blockIdx.x, scan_ar_[arr_addr], carry_ar_[cuda.blockIdx.x]))&#10;                scan_ar_[arr_addr] = scan_ar_[arr_addr] + carry_ar_[cuda.blockIdx.x]&#10;                #print ('scan_ar_[{0}] = {1}'.format (arr_addr, scan_ar_[arr_addr]))&#10;&#10;        # Extract non zero weights from d_weight_ar and place them&#10;        # into the array d_out.&#10;        @cuda.jit&#10;        def extract_nonzero (d_weight_ar, weight_sz, d_scan_ar, __d_out, __nfs_sq):&#10;            thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;            #print ('thread id: {0}, weight_sz: {1}'.format(thid, weight_sz))&#10;            if thid &lt; weight_sz and d_weight_ar[thid] &gt; 0.0:&#10;                # Populate d_out in the correct format:&#10;                src_idx = thid%__nfs_sq&#10;                dst_idx = thid//__nfs_sq&#10;                jj = d_scan_ar[thid]&#10;                __d_out[jj][0] = src_idx&#10;                __d_out[jj][1] = dst_idx&#10;                __d_out[jj][2] = 0.0 # delay - unused&#10;                __d_out[jj][3] = d_weight_ar[thid]&#10;&#10;        # END of kernel definitions&#10;&#10;        # reduce_nonzero_gpu code proper starts:&#10;        print ('--- reduce_nonzero_gpu ---')&#10;        print ('threadsperblock: {0}'.format(threadsperblock))&#10;        print ('arrayszplus: {0}'.format(arrayszplus))&#10;        print ('arraysz: {0}'.format(arraysz)) # Set by code above - size of the weight array. Quite big&#10;&#10;        #&#10;        # Build input data for the test&#10;        #&#10;&#10;        blockspergrid = math.ceil(arraysz/threadsperblock)&#10;        print ('blockspergrid: {0}'.format(blockspergrid))&#10;&#10;        # To pad the arrays out to exact number of blocks&#10;        # arraysz1 = nfs_sq&#10;        #if arraysz%threadsperblock &gt; 0:&#10;        #    amod = arraysz%threadsperblock&#10;        #    print ('--\namod: {0}'.format(amod))&#10;        #    print ('arraysz: {0}'.format(arraysz))&#10;        #    print ('threadsperblock: {0}'.format(threadsperblock))&#10;        #    arrayszplus = arraysz + threadsperblock - amod&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;        #else:&#10;        #    print ('Set arrayszplus to arraysz1...')&#10;        #    arrayszplus = arraysz&#10;        #    print ('arrayszplus: {0}'.format(arrayszplus))&#10;&#10;        # nonzero_ar is set to 1 for every element for which weight_ar is &gt;0&#10;        print ('allocate arrayszplus={0} uint32s in nonzero_ar'.format(arrayszplus))&#10;        nonzero_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # scan_ar is going to hold the result of scanning the input&#10;        scan_ar = np.zeros((arrayszplus,), dtype=np.uint32)&#10;&#10;        # Explicitly copy working data to device (two lots of arraysz data on GPU memory)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_nonzero_ar)&quot;)&#10;        d_nonzero_ar = cuda.to_device (nonzero_ar)&#10;        print (&quot;Allocating &quot; + str(4*arrayszplus/1048576) + &quot; MBytes on the GPU memory (d_scan_ar)&quot;)&#10;        d_scan_ar = cuda.to_device (scan_ar)&#10;&#10;        # Make up a list of carry vectors and allocate device memory&#10;        carrylist = []&#10;        d_carrylist = []&#10;        # And containers for the scan&#10;        scanlist = []&#10;        d_scanlist = []&#10;        asz = arraysz&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;&#10;            carrysz = math.ceil (asz / threadsperblock)&#10;            # Ensure carrysz is a multiple of threadsperblock:&#10;            if carrysz%threadsperblock:&#10;                carrysz = carrysz + threadsperblock - carrysz%threadsperblock&#10;&#10;            print (&quot;Allocating &quot; + str(4*carrysz/1024) + &quot; KBytes on the GPU memory (carrylist)&quot;)&#10;            carrylist.append (np.zeros((carrysz,), dtype=np.float32))&#10;            d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;            asz = math.ceil (asz / threadsperblock)&#10;            scansz = asz&#10;            if scansz%threadsperblock:&#10;                scansz = scansz + threadsperblock - scansz%threadsperblock&#10;            print (&quot;Allocating &quot; + str(4*scansz/1024) + &quot; KBytes on the GPU memory (scanlist)&quot;)&#10;            scanlist.append (np.zeros((scansz,), dtype=np.float32))&#10;            d_scanlist.append (cuda.to_device(scanlist[-1]))&#10;&#10;        # Add a last carrylist, as this will be required as a dummy carry list for the last call to one_block_scan()&#10;        carrylist.append (np.zeros((1,), dtype=np.int32))&#10;        d_carrylist.append (cuda.to_device(carrylist[-1]))&#10;&#10;        #&#10;        # Compute partial scans of the top-level weight_ar and the lower level&#10;        # partial sums&#10;        #&#10;        # The first input is the weight array, compute block-wise prefix-scan sums:&#10;        detect_nonzero[blockspergrid, threadsperblock] (d_weight_ar, d_nonzero_ar, arraysz)&#10;        print ('First one_block_scan...')&#10;        one_block_scan[blockspergrid, threadsperblock] (d_scan_ar, d_nonzero_ar, d_carrylist[0], threadsperblock, arrayszplus)&#10;&#10;        asz = math.ceil (arrayszplus / threadsperblock)&#10;        j = 0&#10;        print ('asz: {0} threadsperblock: {1}'.format(asz, threadsperblock))&#10;        while asz &gt; threadsperblock:&#10;            scanblocks = math.ceil (asz / threadsperblock)&#10;            scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;            print ('while loop one_block_scan...')&#10;            one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;            asz = scanblocks&#10;            j = j+1&#10;        # Plus one more iteration:&#10;        scanblocks = math.ceil (asz / threadsperblock)&#10;        scanblocks = scanblocks + threadsperblock - scanblocks%threadsperblock&#10;        print ('last one_block_scan. scanblock: {0} threadsperblock: {1}, j is {2}'.format(scanblocks, threadsperblock, j))&#10;        one_block_scan[scanblocks, threadsperblock](d_scanlist[j], d_carrylist[j], d_carrylist[j+1], threadsperblock, len(carrylist[j]))&#10;&#10;        #&#10;        # Construct the scans back up the tree by summing the &quot;carry&quot; into the &quot;scans&quot;&#10;        #&#10;        ns = len(scanlist)&#10;        j = ns&#10;        #print ('j starts at {0}'.format(j))&#10;        while j &gt; 0:&#10;            sumblocks = math.ceil(len(scanlist[j-1])/threadsperblock)&#10;            sum_scans[sumblocks, threadsperblock](d_carrylist[j-1], d_scanlist[j-1], len(scanlist[j-1]), d_carrylist[j])&#10;            # Now d_carrylist[j-1] has had its carrys added from the lower level&#10;            j = j-1&#10;&#10;        # The final sum_scans() call. I sum within d_scan_ar destructively at this point.&#10;        sum_scans_destructively[blockspergrid, threadsperblock](d_scan_ar, arrayszplus, d_carrylist[0])&#10;&#10;        # Get the total number of weights from the final carrylist:&#10;        n_weights = 0&#10;        local_carrylist = d_carrylist[ns].copy_to_host()&#10;        last_cl_len = len(local_carrylist)&#10;        if last_cl_len == 1:&#10;            n_weights = local_carrylist[0]&#10;        else:&#10;            print (&quot;ERROR. Length of last carry list should be 1&quot;)&#10;&#10;        # Finally, in parallel, populate d_out.&#10;        extract_nonzero[blockspergrid, threadsperblock] (d_weight_ar, arraysz, d_scan_ar, _d_out, _nfs_sq)&#10;&#10;        return n_weights # END reduce_nonzero_gpu()&#10;    ###############################################################################&#10;    ### GPU SCAN CODE TO HERE&#10;&#10;    ### CONNECTION FUNCTION-COMPUTING CODE HERE&#10;    #&#10;    @cuda.jit(&quot;void(float32,int32,  int32[:,:],int32[:,:],float32[:],float32, float32,float32, float32,  float32,float32, int32,     int32)&quot;)&#10;    def dowork (M_f_start,  nfs_sq, d_src_ar,  d_dst_ar,  weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,    W_cut,   offsetd0p, offsetd1r):&#10;        # Work out i_src and i_dst based on the 2-D thread index&#10;        i_src, i_dst = cuda.grid(2)&#10;        ##print ('i_src: {0}, i_dst: {1}'.format(i_src, i_dst))&#10;&#10;        if i_src &lt; nfs_sq and i_dst &lt; nfs_sq:&#10;&#10;            # Temporary shared memory for weights&#10;            tmp_w = cuda.shared.array(12288, dtype=float32) # Note - allocating ALL shared memory here.&#10;            ##print ('cuda.threadIdx.y: {0} cuda.blockDim.x: {1} cuda.threadIdx.x: {2}'.format(cuda.threadIdx.y, cuda.blockDim.x, cuda.threadIdx.x))&#10;            myidx = (cuda.threadIdx.y*cuda.blockDim.x + cuda.threadIdx.x)&#10;            offsidx = shifted_idx3 (myidx)&#10;            ##print('myidx: {0}, shifted: {1}'.format(myidx, offsidx))&#10;            tmp_w[offsidx] = float32(0.0)&#10;            tmp_w[offsidx+1] = float32(0.0)&#10;            tmp_w[offsidx+2] = float32(0.0)&#10;            cuda.syncthreads()&#10;            ##print ('After syncthreads: tmp_w[0]:{0} [1]:{1} [2]:{2}'.format(tmp_w[0], tmp_w[1], tmp_w[2]))&#10;&#10;            # Compute the location of d_src_ar, this defines what sigma will be. As r (as opp. to phi) increases, the sigma should increase.&#10;            M_f = float32(nfs)/(E2*math.log(((1+d_src_ar[i_src,1])/(2*E2))+1))&#10;&#10;            # Set some of M_f to 1 to ensure the fan-out starts at around the edge of the foveal region.&#10;            if (1+d_src_ar[i_src,1]) &lt; fovshift:&#10;                ##print ('reset M_f to M_f_start because 1+d_src_ar[i_src={2},1] (1+{0:f}) &lt; fovshift ({1:f})'.format(d_src_ar[i_src,1], fovshift, i_src))&#10;                M_f = M_f_start&#10;&#10;            # Compute modified sigma and 3 times this value&#10;            _sigma = (sigma_m/M_f) - (sigma_m/M_f_start) + sigma_0 # as function of r, aka d_src_ar[1]. M_f is the function of r.&#10;            three_sigma = float32(3.0) * _sigma&#10;            ##print ('i_src: {4} i_dst: {5}. d_src_ar: ({0:f},{1:f}) d_dst_ar: ({2:f},{3:f})'.format(d_src_ar[i_src,0], d_src_ar[i_src,1], d_dst_ar[i_dst,0], d_dst_ar[i_dst,1], i_src, i_dst))&#10;            ##if M_f != M_f_start:&#10;                ##print ('sigma_m: {0:f}, M_f:{1:f}. M_f_start:{2:f}. sigma_0: {3:f} _sigma: {4:f} three_sigma: {5:f}'.format(sigma_m, M_f, M_f_start, sigma_0, _sigma, three_sigma))&#10;&#10;            # in-xy-plane distance (ignore d_src_ar[2]/dstdoc[2])&#10;            xd = (d_src_ar[i_src,0] - d_dst_ar[i_dst,0] + offsetd0p)&#10;            yd = (d_src_ar[i_src,1] - d_dst_ar[i_dst,1] + offsetd1r)&#10;            if abs(xd) &lt; three_sigma and abs(yd) &lt; three_sigma:&#10;                ##print('(abs(xd) abs({0:f}) &lt; three_sigma {1:f} and abs(yd) abs({2:f}) &lt; three_sigma)'.format(xd, yd, three_sigma))&#10;                dist = math.sqrt(math.pow(xd,2) + math.pow(yd,2))&#10;                gauss = math.exp(-0.5*math.pow(dist/_sigma,2))&#10;                #gauss = float32(d_src_ar[i_dst,1])&#10;                ##print ('i_src: {0} i_dst: {1} gauss: {2}'.format(i_src, i_dst, gauss))&#10;                if gauss &gt; W_cut:&#10;                    # Write result into weight_ar&#10;                    ##print('gauss {0:f} &gt; W_cut {1:f} - WRITE RESULTS.'.format(gauss, W_cut))&#10;                    tmp_w[offsidx] = float32(gauss)&#10;                    tmp_w[offsidx+1] = float32(i_src)&#10;                    tmp_w[offsidx+2] = float32(i_dst)&#10;                    ##print('tmp_w[{0}] = {1:f}, tmp_w[{2}] = {3:f}, tmp_w[{4}] = {5:f}'.format(offsidx, tmp_w[offsidx], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2]))&#10;&#10;            # Sync threads, then access device memory with any results&#10;            cuda.syncthreads()&#10;            ##print ('After set tmp_w (myidx={6}): tmp_w[{0}]:{1:f} [{2}]:{3:f} [{4}]:{5:f}'.format(offsidx, tmp_w[offsidx+0], offsidx+1, tmp_w[offsidx+1], offsidx+2, tmp_w[offsidx+2], myidx))&#10;            if cuda.threadIdx.x == 0 and cuda.threadIdx.y == 0:&#10;                tpb = cuda.blockDim.x * cuda.blockDim.y&#10;                ##print ('tpb = {0}. tmp_w[0]:{1} [1]:{2} [2]:{3} [3]:{4}'.format(tpb, tmp_w[0], tmp_w[1], tmp_w[2], tmp_w[3]))&#10;                # Write data from tmp_w to res_ar, but only in ONE thread from the threadblock. Should avoid racing.&#10;                for idx in range(0,tpb): # 512 was hard coded here; changed it for tpb&#10;                    offsidx2 = shifted_idx3 (idx)&#10;                    ##print('myidx: {0}, idx: {1}, shifted: {2}'.format(myidx, idx, offsidx2))&#10;                    theweight = tmp_w[offsidx2] # weight should be the first one, so no +1 or +2&#10;                    #if gt(theweight, W_cut): # Testing makes no difference to speed&#10;                    # Add to weight_ar&#10;                    weight_idx = int32(tmp_w[offsidx2+2])*nfs_sq + int32(tmp_w[offsidx2+1])&#10;                    ##if (theweight &gt; 0.0):&#10;                        ##print ('Set weight_ar[{0}]={1}'.format(weight_idx, theweight))&#10;                    weight_ar[weight_idx] = theweight&#10;                    ##if weight_idx &gt; 0:&#10;                        ##print ('weight_ar[{0}] = {1:f}'.format(weight_idx, theweight))&#10;&#10;        return # end dowork()&#10;&#10;    # Initialise a device array with a value. Use with a 1D grid of 1D threadblocks&#10;    @cuda.jit(&quot;void(uint32[:],uint32,uint32)&quot;)&#10;    def init_array (d_array, d_array_sz, value):&#10;        thid = cuda.threadIdx.x + (cuda.blockIdx.x*cuda.blockDim.x)&#10;        if thid &lt; d_array_sz:&#10;            d_array[thid] = value&#10;&#10;    # Compute once only&#10;    M_f_start=nfs/(E2*math.log((fovshift/(2*E2))+1))&#10;    print('M_f_start: {0:f}'.format(M_f_start))&#10;    nfs_sq = int(nfs*nfs)&#10;&#10;    # Copy srclocs and dstlocs to device memory before starting.&#10;    print('Numpy arrays...')&#10;    src_ar = np.array(srclocs, dtype=np.int32)&#10;    dst_ar = np.array(dstlocs, dtype=np.int32)&#10;    print('cuda.to_device...')&#10;    d_src_ar = cuda.to_device (src_ar)&#10;    d_dst_ar = cuda.to_device (dst_ar)&#10;&#10;    # Device array to have the weights computed into&#10;    weight_sz = int(nfs_sq*nfs_sq) # Take care to cast to int numbers which should not be floats.&#10;    print('cuda.device_array: weight_sz: {0}, dtype is np.float32.'.format(weight_sz))&#10;    d_weight_ar = cuda.device_array ((weight_sz,), dtype=np.float32)&#10;&#10;    # COMPUTE WEIGHTS. Call function to compute weights in d_weight_ar&#10;    threadsperblock = (int(16),int(32)) # 16 warps to a block; 512 threads&#10;    #threadsperblock = (16,2) # 16 warps to a block; 512 threads&#10;    print ('threadsperblock: {0}'.format(threadsperblock))&#10;    blockspergrid = (int(1+(nfs_sq // threadsperblock[0])), int(1+(nfs_sq // threadsperblock[1])))&#10;    print ('blockspergrid: {0}'.format(blockspergrid)) # 157 by 79. Gives 2500 by 2500 computations - that's each of 2500 inputs to each of 2500 outs&#10;&#10;    #                                &quot;void(float32,   int32,  int32[:,:], int32[:,:], float32[:],  float32, float32,float32, float32,  float32, float32,int32,     int32)&#10;    time_start = int(round(time.time() * 1000))&#10;    dowork[blockspergrid, threadsperblock](M_f_start, nfs_sq, d_src_ar,   d_dst_ar,   d_weight_ar, sigma_m, E2,     sigma_0, fovshift, nfs,     W_cut,  _offsetd0p, _offsetd1r)&#10;    time_donework = int(round(time.time() * 1000))&#10;    print (&quot;computed weights after {0} ms&quot;.format(time_donework - time_start));&#10;&#10;    # EXTRACT NONZERO WEIGHTS. For the reduce operation, I adopt a 1-D grid of threadblocks&#10;    threadsperblock = int(128)&#10;    blockspergrid = int(math.ceil(weight_sz/threadsperblock))&#10;    print ('blockspergrid: {0}'.format(blockspergrid))&#10;    if weight_sz%threadsperblock:&#10;        weight_sz_plus = int(weight_sz + threadsperblock - weight_sz%threadsperblock)&#10;    else:&#10;        weight_sz_plus = int(weight_sz)&#10;&#10;    # Allocate device memory for the reduce_nonzero_gpu result. In&#10;    # principle this could be as large as nfs^4, but that can call for&#10;    # too much device memory. A max_n_weights parameter of&#10;    # connectionFunc() is passed in to set out_sz.&#10;    out_sz = int(max_n_weights)&#10;&#10;    # First we'll place the output in device memory&#10;    print (&quot;Allocating &quot; + str(4*4*out_sz/1048576) + &quot; MBytes on the GPU memory (d_out)&quot;)&#10;    d_out = cuda.device_array((out_sz,4), dtype=np.float32)&#10;&#10;    # Reduce it down&#10;    dummy = 0&#10;    print (&quot;Reduce down to just the non-zero weights&quot;)&#10;    n_weights = reduce_nonzero_gpu(d_weight_ar, weight_sz, weight_sz_plus, threadsperblock, d_out, nfs_sq)&#10;    time_reduced = int(round(time.time() * 1000))&#10;    print (&quot;Completed reduce down after {0} ms. n_weights={1}&quot;.format(time_reduced-time_donework, n_weights))&#10;&#10;    # Copy the device memory back with the result.&#10;    out = d_out.copy_to_host()&#10;&#10;    time_arraycreated = int(round(time.time() * 1000))&#10;    print (&quot;Got final result after {0} ms&quot;.format(time_arraycreated-time_reduced))&#10;&#10;    print (&quot;Total number of non-zero weights: {0}. out_sz: {1}.&quot;.format (n_weights, out_sz))&#10;    if n_weights &gt; max_n_weights:&#10;        print (&quot;---------------\nWARNING WARNING:\n---------------\nUser chose {0} as max_n_weights, but {1} weights were generated.\nMemory corruption may have occurred!!\n---------------&quot;.format(max_n_weights, n_weights))&#10;&#10;    # Truncate out at length n_weights. Note we're returning a Numpy&#10;    # array cast to a list.&#10;    return out[0:n_weights,:].tolist() # end connectionFunc&#10;#######################################################################&#10;" name="offset_retgauss_gpu" sigma_m="15" E2="2.5" sigma_0="0.3" fovshift="1" nfs="150" W_cut="0.001" offsetd0p="0" offsetd1r="4" max_n_weights="2e+7"/>
                            <Config weightProperty="w"/>
                        </SpineCreator>
                    </LL:Annotation>
                    <BinaryFile file_name="conn_V1_r_edges_to_V2_pMp_rPr_syn0.bin" num_connections="641100" explicit_delay_flag="0" packed_data="true"/>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </ConnectionList>
                <LL:WeightUpdate name="V1_r_edges to V2_pMp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <ValueList>
                            <BinaryFile file_name="explicitDataBinaryFile13.bin" num_elements="641100"/>
                        </ValueList>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V1_r_edges to V2_pMp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
    </LL:Population>
    <LL:Population>
        <LL:Annotation>
            <SpineCreator>
                <xPos value="-6.16069"/>
                <yPos value="6.62624"/>
                <animSpeed value="0.2"/>
                <aspectRatio value="1.66667"/>
                <colour red="0" green="170" blue="255"/>
                <size value="1"/>
                <tag value="3"/>
                <x3D value="0"/>
                <y3D value="0"/>
                <z3D value="0"/>
                <is_visualised value="0"/>
            </SpineCreator>
        </LL:Annotation>
        <LL:Neuron name="V2_p_lines" size="22500" url="LINtanh.xml">
            <Property name="tau" dimension="?">
                <FixedValue value="10"/>
            </Property>
            <Property name="noise" dimension="?">
                <FixedValue value="0.01"/>
            </Property>
            <Property name="a" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </LL:Neuron>
        <Layout url="grid.xml" seed="123" minimum_distance="0">
            <Property name="numNeurons" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="rowlen" dimension="?">
                <FixedValue value="150"/>
            </Property>
            <Property name="x" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="z" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="t" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </Layout>
        <LL:Projection dst_population="V2_pPp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-5.32735" y="6.3801"/>
                    <curves>
                        <curve>
                            <C1 xpos="-4.40788" ypos="6.10854"/>
                            <C2 xpos="-4.69056" ypos="5.46462"/>
                            <end xpos="-3.09303" ypos="5.57631"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_p_lines to V2_pPp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_p_lines to V2_pPp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_r_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-6.15078" y="7.12623"/>
                    <curves>
                        <curve>
                            <C1 xpos="-6.02712" ypos="10.4869"/>
                            <C2 xpos="5.44692" ypos="10.5177"/>
                            <end xpos="5.69438" ypos="7.18322"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_p_lines to V2_r_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_p_lines to V2_r_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-5.32693" y="6.99997"/>
                    <curves>
                        <curve>
                            <C1 xpos="-4.15809" ypos="6.99997"/>
                            <C2 xpos="-4.16864" ypos="7.69685"/>
                            <end xpos="-3.10416" ypos="7.72699"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_p_lines to V2_pPp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_p_lines to V2_pPp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-5.70236" y="7.12623"/>
                    <curves>
                        <curve>
                            <C1 xpos="-3.81812" ypos="8.8133"/>
                            <C2 xpos="-0.865372" ypos="9.84036"/>
                            <end xpos="1.69323" ypos="8.14492"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_p_lines to V2_pMp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_p_lines to V2_pMp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-5.32735" y="6.68332"/>
                    <curves>
                        <curve>
                            <C1 xpos="-1.57961" ypos="6.94006"/>
                            <C2 xpos="-1.35589" ypos="6.75753"/>
                            <end xpos="1.05323" ypos="5.86346"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_p_lines to V2_pMp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_p_lines to V2_pMp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
    </LL:Population>
    <LL:Population>
        <LL:Annotation>
            <SpineCreator>
                <xPos value="5.52935"/>
                <yPos value="6.68322"/>
                <animSpeed value="0.2"/>
                <aspectRatio value="1.66667"/>
                <colour red="0" green="170" blue="255"/>
                <size value="1"/>
                <tag value="4"/>
                <x3D value="0"/>
                <y3D value="0"/>
                <z3D value="0"/>
                <is_visualised value="0"/>
            </SpineCreator>
        </LL:Annotation>
        <LL:Neuron name="V2_r_lines" size="22500" url="LINtanh.xml">
            <Property name="tau" dimension="?">
                <FixedValue value="10"/>
            </Property>
            <Property name="noise" dimension="?">
                <FixedValue value="0.01"/>
            </Property>
            <Property name="a" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </LL:Neuron>
        <Layout url="grid.xml" seed="123" minimum_distance="0">
            <Property name="numNeurons" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="rowlen" dimension="?">
                <FixedValue value="150"/>
            </Property>
            <Property name="x" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="z" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="t" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </Layout>
        <LL:Projection dst_population="V2_pPp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="4.69601" y="6.60313"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.252264" ypos="6.66589"/>
                            <C2 xpos="0.66474" ypos="6.12486"/>
                            <end xpos="-1.42637" ypos="5.69331"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_r_lines to V2_pPp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_r_lines to V2_pPp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_p_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="5.47551" y="7.18322"/>
                    <curves>
                        <curve>
                            <C1 xpos="5.11108" ypos="10.5674"/>
                            <C2 xpos="-5.82664" ypos="9.77746"/>
                            <end xpos="-5.88892" ypos="7.12623"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_r_lines to V2_p_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_r_lines to V2_p_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="5.12856" y="7.18322"/>
                    <curves>
                        <curve>
                            <C1 xpos="2.65248" ypos="9.44589"/>
                            <C2 xpos="0.69926" ypos="9.1983"/>
                            <end xpos="-1.43765" ypos="8.13224"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_r_lines to V2_pPp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_r_lines to V2_pPp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="4.84582" y="7.18322"/>
                    <curves>
                        <curve>
                            <C1 xpos="3.93844" ypos="7.858"/>
                            <C2 xpos="3.32405" ypos="7.64709"/>
                            <end xpos="2.70073" ypos="7.59701"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_r_lines to V2_pMp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_r_lines to V2_pMp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="4.69601" y="6.32899"/>
                    <curves>
                        <curve>
                            <C1 xpos="3.57204" ypos="5.85123"/>
                            <C2 xpos="3.43745" ypos="5.57502"/>
                            <end xpos="2.71989" ypos="5.56538"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_r_lines to V2_pMp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_r_lines to V2_pMp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
    </LL:Population>
    <LL:Population>
        <LL:Annotation>
            <SpineCreator>
                <xPos value="-2.2597"/>
                <yPos value="5.64418"/>
                <animSpeed value="0.2"/>
                <aspectRatio value="1.66667"/>
                <colour red="0" green="170" blue="255"/>
                <size value="1"/>
                <tag value="6"/>
                <x3D value="0"/>
                <y3D value="0"/>
                <z3D value="0"/>
                <is_visualised value="0"/>
            </SpineCreator>
        </LL:Annotation>
        <LL:Neuron name="V2_pPp_rPr" size="22500" url="LINtanh.xml">
            <Property name="tau" dimension="?">
                <FixedValue value="10"/>
            </Property>
            <Property name="noise" dimension="?">
                <FixedValue value="0.01"/>
            </Property>
            <Property name="a" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </LL:Neuron>
        <Layout url="grid.xml" seed="123" minimum_distance="0">
            <Property name="numNeurons" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="rowlen" dimension="?">
                <FixedValue value="150"/>
            </Property>
            <Property name="x" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="z" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="t" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </Layout>
        <LL:Projection dst_population="V2_p_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-3.09303" y="5.43308"/>
                    <curves>
                        <curve>
                            <C1 xpos="-3.93465" ypos="5.21988"/>
                            <C2 xpos="-4.20454" ypos="5.54714"/>
                            <end xpos="-5.32735" ypos="6.27329"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pPp_rPr to V2_p_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pPp_rPr to V2_p_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_r_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-1.42637" y="5.82741"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.0123252" ypos="6.08876"/>
                            <C2 xpos="-0.100511" ypos="6.77502"/>
                            <end xpos="4.69543" ypos="6.7136"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pPp_rPr to V2_r_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pPp_rPr to V2_r_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-2.77853" y="6.14418"/>
                    <curves>
                        <curve>
                            <C1 xpos="-2.7752" ypos="6.76278"/>
                            <C2 xpos="-2.93906" ypos="6.69528"/>
                            <end xpos="-2.90464" ypos="7.25067"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pPp_rPr to V2_pPp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pPp_rPr to V2_pPp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-1.868" y="6.14418"/>
                    <curves>
                        <curve>
                            <C1 xpos="-1.26098" ypos="6.91903"/>
                            <C2 xpos="-0.0075433" ypos="7.25842"/>
                            <end xpos="1.03405" ypos="7.47313"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pPp_rPr to V2_pMp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pPp_rPr to V2_pMp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-1.42637" y="5.3853"/>
                    <curves>
                        <curve>
                            <C1 xpos="0.133683" ypos="4.90067"/>
                            <C2 xpos="0.304408" ypos="5.12014"/>
                            <end xpos="1.05323" ypos="5.32557"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pPp_rPr to V2_pMp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pPp_rPr to V2_pMp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
    </LL:Population>
    <LL:Population>
        <LL:Annotation>
            <SpineCreator>
                <xPos value="-2.27099"/>
                <yPos value="7.75092"/>
                <animSpeed value="0.2"/>
                <aspectRatio value="1.66667"/>
                <colour red="0" green="170" blue="255"/>
                <size value="1"/>
                <tag value="2"/>
                <x3D value="0"/>
                <y3D value="0"/>
                <z3D value="0"/>
                <is_visualised value="0"/>
            </SpineCreator>
        </LL:Annotation>
        <LL:Neuron name="V2_pPp_rMr" size="22500" url="LINtanh.xml">
            <Property name="tau" dimension="?">
                <FixedValue value="10"/>
            </Property>
            <Property name="noise" dimension="?">
                <FixedValue value="0.01"/>
            </Property>
            <Property name="a" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </LL:Neuron>
        <Layout url="grid.xml" seed="123" minimum_distance="0">
            <Property name="numNeurons" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="rowlen" dimension="?">
                <FixedValue value="150"/>
            </Property>
            <Property name="x" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="z" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="t" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </Layout>
        <LL:Projection dst_population="V2_p_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-3.10414" y="7.50843"/>
                    <curves>
                        <curve>
                            <C1 xpos="-4.26574" ypos="7.50843"/>
                            <C2 xpos="-4.26554" ypos="6.86725"/>
                            <end xpos="-5.32696" ypos="6.86725"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pPp_rMr to V2_p_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pPp_rMr to V2_p_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-2.67293" y="7.25069"/>
                    <curves>
                        <curve>
                            <C1 xpos="-2.74146" ypos="6.39257"/>
                            <C2 xpos="-2.59861" ypos="6.95619"/>
                            <end xpos="-2.6145" ypos="6.14418"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pPp_rMr to V2_pPp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pPp_rMr to V2_pPp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_r_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-1.50885" y="8.25092"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.144109" ypos="9.14626"/>
                            <C2 xpos="2.4941" ypos="9.84978"/>
                            <end xpos="5.28108" ypos="7.18322"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pPp_rMr to V2_r_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pPp_rMr to V2_r_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-1.43765" y="7.98291"/>
                    <curves>
                        <curve>
                            <C1 xpos="0.0982825" ypos="8.15303"/>
                            <C2 xpos="-0.410834" ypos="7.95504"/>
                            <end xpos="1.03405" ypos="7.89425"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pPp_rMr to V2_pMp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pPp_rMr to V2_pMp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="-1.43765" y="7.48016"/>
                    <curves>
                        <curve>
                            <C1 xpos="0.22654" ypos="6.93943"/>
                            <C2 xpos="0.314118" ypos="6.58484"/>
                            <end xpos="1.21399" ypos="6.05419"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pPp_rMr to V2_pMp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pPp_rMr to V2_pMp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
    </LL:Population>
    <LL:Population>
        <LL:Annotation>
            <SpineCreator>
                <xPos value="1.88656"/>
                <yPos value="5.55419"/>
                <animSpeed value="0.2"/>
                <aspectRatio value="1.66667"/>
                <colour red="0" green="170" blue="255"/>
                <size value="1"/>
                <tag value="3"/>
                <x3D value="0"/>
                <y3D value="0"/>
                <z3D value="-10"/>
                <is_visualised value="1"/>
            </SpineCreator>
        </LL:Annotation>
        <LL:Neuron name="V2_pMp_rPr" size="22500" url="LINtanh.xml">
            <Property name="tau" dimension="?">
                <FixedValue value="10"/>
            </Property>
            <Property name="noise" dimension="?">
                <FixedValue value="0.01"/>
            </Property>
            <Property name="a" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </LL:Neuron>
        <Layout url="grid.xml" seed="123" minimum_distance="0">
            <Property name="numNeurons" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="rowlen" dimension="?">
                <FixedValue value="150"/>
            </Property>
            <Property name="x" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="z" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="t" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </Layout>
        <LL:Projection dst_population="V2_pMp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="2.4002" y="6.05419"/>
                    <curves>
                        <curve>
                            <C1 xpos="2.5973" ypos="6.77886"/>
                            <C2 xpos="2.35193" ypos="6.48988"/>
                            <end xpos="2.42263" ypos="7.14492"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pMp_rPr to V2_pMp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pMp_rPr to V2_pMp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="1.36841" y="6.05419"/>
                    <curves>
                        <curve>
                            <C1 xpos="0.613102" ypos="6.78305"/>
                            <C2 xpos="-0.646231" ypos="7.41938"/>
                            <end xpos="-1.43765" ypos="7.58088"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pMp_rPr to V2_pPp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pMp_rPr to V2_pPp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="1.05323" y="5.17306"/>
                    <curves>
                        <curve>
                            <C1 xpos="0.092472" ypos="4.86637"/>
                            <C2 xpos="-0.605472" ypos="4.93784"/>
                            <end xpos="-1.42637" ypos="5.27913"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pMp_rPr to V2_pPp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pMp_rPr to V2_pPp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_r_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="2.71989" y="5.74122"/>
                    <curves>
                        <curve>
                            <C1 xpos="4.04443" ypos="5.74122"/>
                            <C2 xpos="4.04443" ypos="6.40381"/>
                            <end xpos="4.69601" ypos="6.40381"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pMp_rPr to V2_r_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pMp_rPr to V2_r_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_p_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="1.05323" y="5.70149"/>
                    <curves>
                        <curve>
                            <C1 xpos="-2.43473" ypos="7.08772"/>
                            <C2 xpos="-2.50808" ypos="6.64754"/>
                            <end xpos="-5.32735" ypos="6.5194"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pMp_rPr to V2_p_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pMp_rPr to V2_p_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
    </LL:Population>
    <LL:Population>
        <LL:Annotation>
            <SpineCreator>
                <xPos value="1.86739"/>
                <yPos value="7.64492"/>
                <animSpeed value="0.2"/>
                <aspectRatio value="1.66667"/>
                <colour red="0" green="170" blue="255"/>
                <size value="1"/>
                <tag value="4"/>
                <x3D value="0"/>
                <y3D value="0"/>
                <z3D value="0"/>
                <is_visualised value="0"/>
            </SpineCreator>
        </LL:Annotation>
        <LL:Neuron name="V2_pMp_rMr" size="22500" url="LINtanh.xml">
            <Property name="tau" dimension="?">
                <FixedValue value="10"/>
            </Property>
            <Property name="noise" dimension="?">
                <FixedValue value="0.01"/>
            </Property>
            <Property name="a" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </LL:Neuron>
        <Layout url="grid.xml" seed="123" minimum_distance="0">
            <Property name="numNeurons" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="rowlen" dimension="?">
                <FixedValue value="150"/>
            </Property>
            <Property name="x" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="y" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="z" dimension="?">
                <FixedValue value="0"/>
            </Property>
            <Property name="t" dimension="?">
                <FixedValue value="0"/>
            </Property>
        </Layout>
        <LL:Projection dst_population="V2_r_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="2.70073" y="7.73779"/>
                    <curves>
                        <curve>
                            <C1 xpos="3.75929" ypos="7.85577"/>
                            <C2 xpos="4.2081" ypos="7.87636"/>
                            <end xpos="4.97566" ypos="7.18322"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pMp_rMr to V2_r_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pMp_rMr to V2_r_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pMp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="2.25565" y="7.14492"/>
                    <curves>
                        <curve>
                            <C1 xpos="2.19381" ypos="6.32085"/>
                            <C2 xpos="2.46098" ypos="6.83339"/>
                            <end xpos="2.25271" ypos="6.05419"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pMp_rMr to V2_pMp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pMp_rMr to V2_pMp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rPr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="1.03405" y="7.38524"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.33434" ypos="6.95882"/>
                            <C2 xpos="-0.884755" ypos="6.9706"/>
                            <end xpos="-1.74141" ypos="6.14418"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pMp_rMr to V2_pPp_rPr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pMp_rMr to V2_pPp_rPr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_pPp_rMr">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="1.03405" y="7.73824"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.262342" ypos="7.76412"/>
                            <C2 xpos="0.041714" ypos="7.98333"/>
                            <end xpos="-1.43765" ypos="7.88032"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pMp_rMr to V2_pPp_rMr Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pMp_rMr to V2_pPp_rMr Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
        <LL:Projection dst_population="V2_p_lines">
            <LL:Annotation>
                <SpineCreator>
                    <DrawOptions style="0" showlabel="0"/>
                    <start x="1.46221" y="8.14492"/>
                    <curves>
                        <curve>
                            <C1 xpos="-0.581101" ypos="9.27182"/>
                            <C2 xpos="-3.32692" ypos="8.76576"/>
                            <end xpos="-5.49844" ypos="7.12623"/>
                        </curve>
                    </curves>
                </SpineCreator>
            </LL:Annotation>
            <LL:Synapse>
                <OneToOneConnection>
                    <Delay dimension="ms">
                        <FixedValue value="1"/>
                    </Delay>
                </OneToOneConnection>
                <LL:WeightUpdate name="V2_pMp_rMr to V2_p_lines Synapse 0 weight_update" url="Weight.xml" input_src_port="y" input_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="-0.6"/>
                    </Property>
                </LL:WeightUpdate>
                <LL:PostSynapse name="V2_pMp_rMr to V2_p_lines Synapse 0 postsynapse" url="passthrough.xml" input_src_port="out" input_dst_port="in" output_src_port="out" output_dst_port="in">
                    <Property name="w" dimension="?">
                        <FixedValue value="1"/>
                    </Property>
                </LL:PostSynapse>
            </LL:Synapse>
        </LL:Projection>
    </LL:Population>
</LL:SpineML>
